<p align="center">
    <img src="logo.png" width="400"/>
<p>
<p align="center">
    <b><font size="6">InternLM-XComposer</font></b>
<p>

<!-- <div align="center">
        InternLM-XComposer <a href="">ğŸ¤– <a> <a href="">ğŸ¤—</a>&nbsp ï½œ InternLM-VL <a href="">ğŸ¤– <a> <a href="">ğŸ¤—</a>&nbsp | Technical Report <a href=""> <a> ğŸ“„  -->

<div align="center">
        InternLM-XComposer <a href="">ğŸ¤—</a>&nbsp ï½œ InternLM-XComposer-VL <a href="">ğŸ¤—</a>&nbsp | Technical Report <a href="">  ğŸ“„ <a>

<!-- [English](./README.md) | [ç®€ä½“ä¸­æ–‡](./README_zh.md) -->

</div>

<br>




**InternLM-XComposer** is a vision-language large model (VLLM) based on [InternLM](https://github.com/InternLM/InternLM/tree/main) for advanced text-image comprehension and composition. InternLM-XComposer has serveal appealing properties:

- **Interleaved Text-Image Composition**: InternLM-XComposer can effortlessly generate coherent and contextual articles that seamlessly integrate images, providing a more engaging and immersive reading experience. The interleaved text-image composition is implemented in following steps:

    1. **Text Generation**: It crafts long-form text based on human-provided instructions.
    2. **Image Spoting and Captioning**: It pinpoints optimal locations for image placement and furnishes image descriptions.
    3. **Image Retrieval and Selection**: It select image candidates and identify the image that optimally complements the content.

- **Comprehension with Rich Multilingual Knowledge**: The text-image comprehension is empowered by training on extensive multi-modal multilingual concepts with carefully crafted strategies, resulting in a deep understanding of visual content.
- **Strong performance**: It consistently achieves state-of-the-art results across various benchmarks for vision-language large models, including [MME Benchmark](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) (English), [MMBench](https://opencompass.org.cn/leaderboard-multimodal) (English), [Seed-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard) (English), [CCBench](https://opencompass.org.cn/leaderboard-multimodal)(Chinese), and [MMBench-CN](https://opencompass.org.cn/leaderboard-multimodal) (Chineese).

We release InternLM-XComposer series in two versions:

- [InternLM-XComposer-VL-7B](https://huggingface.co/internlm/internlm-xcomposer-vl-7b): The pretrained VLLM model with InternLM as the initialization of the LLM, achieving strong performance on various multimodal benchmarks, e.g., MME Benchmark, MMBench Seed-Bench, CCBench, and MMBench-CN.
- [InternLM-XComposer-7B](https://huggingface.co/internlm/internlm-xcomposer-7b): The finetuned VLLM for *Interleaved Text-Image Composition* and *LLM-based AI assistant*.
  <br>

<!-- 
<p align="center">
    <figcaption align = "center"><b> InternLM-XComposer </b></figcaption>
<p> -->



## News and Updates
* ```2023.9.27``` ğŸ‰ğŸ‰ğŸ‰ The [evaluation code](./evaluation/) of **InternLM-XComposer-VL-7B** are publicly available.
* ```2023.9.27``` ğŸ‰ğŸ‰ğŸ‰ **InternLM-XComposer-7B** and **InternLM-XComposer-VL-7B** are publicly available on ModelScope and Hugging Face. 
* ```2023.9.27``` ğŸ‰ğŸ‰ğŸ‰ We release a [technical report]() for more details of our model series.
<br>

## Evaluation

We evaluate InternLM-XComposer-VL on five multimodal benchmarks: [MME Benchmark](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation), [MMBench](https://opencompass.org.cn/leaderboard-multimodal), [Seed-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard) in the English language, [CCBench](https://opencompass.org.cn/leaderboard-multimodal), [MMBench-CN](https://opencompass.org.cn/leaderboard-multimodal) in the simplified chinese language.

   - [MME Benchmark](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation): A comprehensive evaluation benchmark for multimodal large language models with 14 subtasks.
   - [MMBench](https://opencompass.org.cn/leaderboard-multimodal): A comprehensive evaluation pipeline comprised of meticulously curated multimodal dataset and a novel circulareval strategy using ChatGPT.
   - [MMBench-CN](https://opencompass.org.cn/leaderboard-multimodal): A simplified chinese language version of [MMBench](https://opencompass.org.cn/leaderboard-multimodal).
   - [Seed-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard): A multimodal benchmark of 19K multiple-choice questions with accurate human annotations for evaluating Multimodal LLMs.
   - [CCBench](): A multimodal benchmark for chinese cultural comprehension.

InternLM-XComposer-VL outperforms existing vision-language large models on **all the five benchmarks**, demonstrating stronger multilingual comprehension ability.


### MME Benchmark

[MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) is a comprehensive evaluation benchmark for multimodal large language models. It measures both perception and cognition abilities on a total of 14 subtasks, including existence, count, position, color, poster, celebrity, scene, landmark, artwork, OCR, commonsense reasoning, numerical calculation, text translation, and code reasoning.

InternLM-XComposer-VL achieves SOTAs on overall performance evaluation. See more details on [HERE](evaluation/mme/MME_Bench.md).


<p align="center">
Overall Performance
<p>


<center>

| Rank |      Model      |          Version         |  Score  |
|:----:|:---------------:|:------------------------:|:-------:|
| ï¸  1  | [InternLM-XComposer-VL](https://github.com/InternLM/InternLM-XComposer) | [InternLM-7B](https://github.com/InternLM/InternLM-XComposer) | 1919.5 |
|   2  | Qwen-VL-Chat    |        Qwen-7B            | 1848.3 |
|   3  |      MMICL      |         FlanT5xxl        | 1810.7 |
|   4  |    Skywork-MM   |      Skywork-MM-13B      | 1775.5 |
|   5  |       BLIVA     |    FlanT5xxl             | 1669.2 |

</center>



<p align="center">
    <img src="evaluation/mme/perception.PNG" width="600"/>
<p>
<p align="center">
    <img src="evaluation/mme/cognition.PNG" width="600"/>
<p>


### MMBench & MMBench-CN

[MMBench](https://opencompass.org.cn/leaderboard-multimodal) is a comprehensive evaluation pipeline comprised of meticulously curated multimodal dataset and a novel circulareval strategy using ChatGPT. It is comprised of 20 ability dimensions defined by MMBench. 

InternLM-XComposer-VL achieves SOTAs on both test and dev split. See more details on [HERE](evaluation/mmbench/MMBench.md).


<p align="center">
MMBench Test Split
<p>

<center>

| Rank |      Model      |          Version         |  Score  |
|:----:|:---------------:|:------------------------:|:-------:|
| ï¸  1  | InternLM-XComposer-VL | InternLM-7B | 74.4 |
|   2  |    Pink  |        Vicuna-7B            | 74.1 |
|   3  |      JiuTian      |        FLANT5-XXL        | 71.8 |
|   4  |  WeMM   |      InternLM-7B      | 69.0 |
|   5  |     mPLUG-Owl     |    LLaMA2 7B            |  68.5 |

</center>

<p align="center">
    <img src="evaluation/mmbench/mmbench.PNG" width="1000"/>
<p>

<p align="center">
MMBench-CN Test Split
<p>

<center>

| Rank |      Model      |          Version         |  Score  |
|:----:|:---------------:|:------------------------:|:-------:|
| ï¸  1  | InternLM-XComposer-VL | InternLM-7B | 72.4 |
|   2  |    QWen-VL-Chat | Qwen-7B | 56.3 |
|   3  |    LLaVA       | LLaMA 7B  |36.6 |
|   4  |    VosualGLM   | ChatGLM 6B | 25.6 |
|   5  |    mPLUG-Owl | LLaMA2 7B  | 24.9 |

</center>

<p align="center">
    <img src="evaluation/mmbench/mmbench_cn.PNG" width="1000"/>
<p>

### SEED-Bench

[SEED-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard) is a multimodal benchmark of 19K multiple-choice questions with accurate human annotations for evaluating Multimodal LLMs, covering 12 evaluation dimensions including both **image** and **video** understanding. See more details on [HERE](evaluation/seed_bench/SEED.md).

InternLM-VL achieves SOTAs on this benchmark for images.


<p align="center">
SeedBench Image Evaluation
<p>

<center>

| Rank |      Model      |          Version         |  Score  |
|:----:|:---------------:|:------------------------:|:-------:|
| ï¸  1  | InternLM-XComposer-VL | InternLM-7B | 66.9 |
|   2  |    QWen-VL-Chat | Qwen-7B | 65.4 |
|   3  |    QWen-VL | Qwen-7B | 62.3 |
|   4  |    InstructBLIP-Vicuna   |        Vicuna 7B  | 58.8 |
|   5  |    InstructBLIP   |     Flan-T5-XL  | 57.8 |

</center>

<p align="center">
    <img src="evaluation/seed_bench/seed_bench.PNG" width="1000"/>
<p>

### CCBench

[CCBench](https://opencompass.org.cn/leaderboard-multimodal) is a multimodal benchmark for chinese cultural comprehension. See more details on [HERE](evaluation/seed_bench/MMBench.md).

<p align="center">
CCBench Performance
<p>

<center>

| Rank |      Model      |          Version         |  Score  |
|:----:|:---------------:|:------------------------:|:-------:|
| ï¸  1  | InternLM-XComposer-VL | InternLM-7B | 47.6 |
|   2  |    QWen-VL-Chat | Qwen-7B | 39.3 |
|   3  |    mPLUG-Owl | LLaMA2 7B  | 12.9 |
|   3  |    InstructBLIP       |        Vicuna 7B  | 12.1 |
|   4  |    VosualGLM   | ChatGLM 6B | 9.2  |

</center>

<p align="center">
    <img src="evaluation/mmbench/ccbench.PNG" width="1000"/>
<p>

## Requirements

* python 3.8 and above
* pytorch 1.12 and above, 2.0 and above are recommended
* CUDA 11.4 and above are recommended (this is for GPU users)
  <br>

## Installation

Before running the code, make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries.
Please refer to the [installation instructions](docs/install.md)

## Quickstart

We provide a simple example to show how to use InternLM-XComposer with ğŸ¤— Transformers.

#### ğŸ¤— Transformers

```python
import torch
from transformers import AutoModel, AutoTokenizer

torch.set_grad_enabled(False)

# init model and tokenizer
model = AutoModel.from_pretrained('internlm/internlm-xcomposer-7b', trust_remote_code=True).cuda().eval()
tokenizer = AutoTokenizer.from_pretrained('internlm/internlm-xcomposer-7b', trust_remote_code=True)
model.tokenizer = tokenizer

# example image
image = 'examples/images/aiyinsitan.jpg'

# Single-Turn Pure-Text Dialogue
text = 'è¯·ä»‹ç»ä¸‹çˆ±å› æ–¯å¦çš„ç”Ÿå¹³'
response = model.generate(text)
# 'é˜¿å°”ä¼¯ç‰¹Â·çˆ±å› æ–¯å¦ï¼ˆAlbert Einsteinï¼Œ1879å¹´3æœˆ14æ—¥ï¼1955å¹´4æœˆ18æ—¥ï¼‰ï¼Œå¾·å›½è£”ç‘å£«ç±ç‰©ç†å­¦å®¶ã€‚ä»–åˆ›ç«‹äº†ç°ä»£ç‰©ç†å­¦çš„ä¸¤å¤§æ”¯æŸ±ç†è®ºï¼š
# ç›¸å¯¹è®ºå’Œé‡å­åŠ›å­¦ï¼Œ è€Œè´¨èƒ½ç­‰ä»·å…¬å¼E=mc2ä¾¿æ˜¯ä»–çš„ç›¸å¯¹è®ºæ€æƒ³çš„æ˜è¯ï¼Œå› è€Œè¢«å…¬è®¤ä¸ºæ˜¯ç»§ä¼½åˆ©ç•¥ã€ç‰›é¡¿ä¹‹åæœ€ä¼Ÿå¤§çš„ç‰©ç†å­¦å®¶ã€‚
# 1999å¹´ï¼Œçˆ±å› æ–¯å¦è¢«ç¾å›½ã€Šæ—¶ä»£å‘¨åˆŠã€‹è¯„é€‰ä¸º20ä¸–çºªçš„â€œä¸–çºªäººç‰©â€ï¼Œä»–åœ¨ç‰©ç†å­¦ä¸Šçš„è´¡çŒ®ï¼Œä½¿ä»–åœ¨ä¸–ç•Œå„åœ°å—åˆ°äººä»¬çš„å°Šæ•¬ã€‚'

# Single-Turn Text-Image Dialogue
text = 'è¯·é—®è¿™å¼ å›¾ç‰‡é‡Œé¢çš„äººæ˜¯è°ï¼Ÿå¹¶ä»‹ç»ä¸‹ä»–ã€‚'
image = 'examples/images/aiyinsitan.jpg'
response = model.generate(text, image)
# å›¾ç‰‡ä¸­çš„ç”·å­æ˜¯é˜¿å°”ä¼¯ç‰¹Â·çˆ±å› æ–¯å¦ï¼ˆAlbert Einsteinï¼‰ï¼Œä¸€ä½è‘—åçš„ç‰©ç†å­¦å®¶å’Œç†è®ºç‰©ç†å­¦å®¶ã€‚ä»–äº1879å¹´3æœˆ14æ—¥å‡ºç”Ÿäºå¾·å›½å·´ç™»-ç¬¦è…¾å ¡å·çš„ä¹Œå°”å§†å¸‚ï¼Œ
# 1955 å¹´4æœˆ18æ—¥é€ä¸–äºç¾å›½æ–°æ³½è¥¿å·æ™®æ—æ–¯é¡¿å¸‚ã€‚çˆ±å› æ–¯å¦åœ¨20ä¸–çºªåˆæå‡ºäº†ç‹­ä¹‰ç›¸å¯¹è®ºå’Œå¹¿ä¹‰ç›¸å¯¹è®ºï¼Œå¯¹ç°ä»£ç‰©ç†å­¦çš„å‘å±•äº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚

# Multi-Turn Text-Image Dialogue
# 1st turn
text = 'å›¾ç‰‡é‡Œé¢çš„æ˜¯è°ï¼Ÿ'
response, history = model.chat(text=text, image=image, history=None)
# é˜¿å°”ä¼¯ç‰¹Â·çˆ±å› æ–¯å¦ã€‚

# 2nd turn
text = 'ä»–æœ‰å“ªäº›æˆå°±?'
response, history = model.chat(text=text, image=None, history=history)
# é˜¿å°”ä¼¯ç‰¹Â·çˆ±å› æ–¯å¦æ˜¯20ä¸–çºªæœ€ä¼Ÿå¤§çš„ç‰©ç†å­¦å®¶ä¹‹ä¸€ï¼Œä»–æå‡ºäº†ç‹­ä¹‰ç›¸å¯¹è®ºå’Œå¹¿ä¹‰ç›¸å¯¹è®ºï¼Œå¯¹ç°ä»£ç‰©ç†å­¦çš„å‘å±•äº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚
# æ­¤å¤–ï¼Œä»–è¿˜æå‡ºäº†è‘—åçš„è´¨èƒ½æ–¹ç¨‹E=mcÂ²ï¼Œä¸ºæ ¸èƒ½çš„å¼€å‘æä¾›äº†ç†è®ºåŸºç¡€ã€‚

# 3rd turn
text = 'ä»–æ˜¯æœ€ä¼Ÿå¤§çš„ç‰©ç†å­¦å®¶å—?'
response, history = model.chat(text=text, image=None, history=history)
# æ˜¯çš„ï¼Œé˜¿å°”ä¼¯ç‰¹Â·çˆ±å› æ–¯å¦æ˜¯20ä¸–çºªæœ€ä¼Ÿå¤§çš„ç‰©ç†å­¦å®¶ä¹‹ä¸€ã€‚
```

## Demo

### Web UI

We provide code for users to build a web UI demo.

Please run the command below:

```
python examples/web_demo.py
```

<br>

## Citation

If you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)

```BibTeX
@article{InternLM-XComposer,
  title={InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition},
  author={Zhang and Pan, Dong and Xiaoyi, Wang and Bin, Cao and Yuhang, Xu and Chao, Ouyang and Linke, Zhao and Zhiyuan, Ding and Shuangrui, Zhang and Songyang, Duan and Haodong, Yan and Hang, Zhang and Xinyue, Li and Wei, Li and Jingwen, Chen and Kai, He and Conghui, Zhang and Xingcheng, Qiao and Yu, Lin and Dahua, Wang and Jiaqi},
  journal={arXiv preprint},
  year={2023}
}
```

<br>

## License & Contact Us

The code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow free commercial usage. To apply for a commercial license, please fill in the application form (English)/ç”³è¯·è¡¨ï¼ˆä¸­æ–‡ï¼‰. For other questions or collaborations, please contact internlm@pjlab.org.cn.
