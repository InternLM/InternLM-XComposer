import os
import re
import sys
sys.path.insert(0, '.')
sys.path.insert(0, '..')

import gradio as gr
os.environ["GRADIO_TEMP_DIR"] = os.path.join(os.getcwd(), 'tmp')
import copy
import time
import shutil
import requests
from PIL import Image, ImageFile
import torch
import transformers
from transformers import StoppingCriteriaList, AutoTokenizer, AutoModel

ImageFile.LOAD_TRUNCATED_IMAGES = True

from demo_asset.assets.css_html_js import custom_css
from demo_asset.gradio_patch import Chatbot as grChatbot
from demo_asset.serve_utils import Stream, Iteratorize
from demo_asset.conversation import CONV_VISION_7132_v2, StoppingCriteriaSub
from demo_asset.download import download_image_thread

max_section = 60
no_change_btn = gr.Button.update()
disable_btn = gr.Button.update(interactive=False)
enable_btn = gr.Button.update(interactive=True)
chat_stream_output = True
article_stream_output = True


def get_urls(caption, exclude):
    headers = {'Content-Type': 'application/json'}
    json_data = {'caption': caption, 'exclude': exclude, 'need_idxs': True}
    response = requests.post('https://lingbi.openxlab.org.cn/image/similar',
                             headers=headers,
                             json=json_data)
    urls = response.json()['data']['image_urls']
    idx = response.json()['data']['indices']
    return urls, idx


class Demo_UI:
    def __init__(self):
        # self.llm_model = AutoModel.from_pretrained(
        #     'internlm/internlm-xcomposer-7b', trust_remote_code=True)
        # tokenizer = AutoTokenizer.from_pretrained(
        #     'internlm/internlm-xcomposer-7b', trust_remote_code=True)
        self.llm_model = AutoModel.from_pretrained(
            '/mnt/petrelfs/share_data/dongxiaoyi/share_models/release_chat', trust_remote_code=True)
        tokenizer = AutoTokenizer.from_pretrained(
            '/mnt/petrelfs/share_data/dongxiaoyi/share_models/release_chat', trust_remote_code=True)

        self.llm_model.internlm_tokenizer = tokenizer
        self.llm_model.tokenizer = tokenizer
        self.llm_model.eval().to('cuda')
        self.device = 'cuda'
        print(f" load model done: ", type(self.llm_model))

        self.eoh = self.llm_model.internlm_tokenizer.decode(
            torch.Tensor([103027]), skip_special_tokens=True)
        self.eoa = self.llm_model.internlm_tokenizer.decode(
            torch.Tensor([103028]), skip_special_tokens=True)
        self.soi_id = len(tokenizer) - 1
        self.soi_token = '<SOI_TOKEN>'

        self.vis_processor = self.llm_model.vis_processor
        self.device = 'cuda'

        stop_words_ids = [
            torch.tensor([943]).to(self.device),
            torch.tensor([2917, 44930]).to(self.device),
            torch.tensor([45623]).to(self.device),  ### new setting
            torch.tensor([46323]).to(self.device),  ### new setting
            torch.tensor([103027]).to(self.device),  ### new setting
            torch.tensor([103028]).to(self.device),  ### new setting
        ]
        self.stopping_criteria = StoppingCriteriaList(
            [StoppingCriteriaSub(stops=stop_words_ids)])
        self.r2 = re.compile(r'<Seg[0-9]*>')
        self.max_txt_len = 1680

    def reset(self):
        self.output_text = ''
        self.caps = {}
        self.show_caps = False
        self.show_ids = {}

    def get_images_xlab(self, caption, loc, exclude):
        urls, idxs = get_urls(caption.strip()[:53], exclude)
        print(urls[0])
        print('download image with url')
        download_image_thread(urls,
                              folder='articles/' + self.title,
                              index=self.show_ids[loc] * 1000 + loc,
                              num_processes=4)
        print('image downloaded')
        return idxs

    def generate(self, text, random, beam, max_length, repetition):
        input_tokens = self.llm_model.internlm_tokenizer(
            text, return_tensors="pt",
            add_special_tokens=True).to(self.llm_model.device)
        img_embeds = self.llm_model.internlm_model.model.embed_tokens(
            input_tokens.input_ids)
        with torch.no_grad():
            with self.llm_model.maybe_autocast():
                outputs = self.llm_model.internlm_model.generate(
                    inputs_embeds=img_embeds,
                    stopping_criteria=self.stopping_criteria,
                    do_sample=random,
                    num_beams=beam,
                    max_length=max_length,
                    repetition_penalty=float(repetition),
                )
        output_text = self.llm_model.internlm_tokenizer.decode(
            outputs[0][1:], add_special_tokens=False)
        output_text = output_text.split('<TOKENS_UNUSED_1>')[0]
        return output_text

    def generate_text(self, title, beam, repetition, text_num, random):
        text = ' <|User|>:æ ¹æ®ç»™å®šæ ‡é¢˜å†™ä¸€ä¸ªå›¾æ–‡å¹¶èŒ‚ï¼Œä¸é‡å¤çš„æ–‡ç« ï¼š{}\n'.format(
            title) + self.eoh + ' <|Bot|>:'
        print('random generate:{}'.format(random))
        output_text = self.generate(text, random, beam, text_num, repetition)
        return output_text

    def generate_loc(self, text_sections, image_num, progress):
        full_txt = ''.join(text_sections)
        input_text = f' <|User|>:ç»™å®šæ–‡ç« "{full_txt}" æ ¹æ®ä¸Šè¿°æ–‡ç« ï¼Œé€‰æ‹©é€‚åˆæ’å…¥å›¾åƒçš„{image_num}è¡Œ' + ' \n<TOKENS_UNUSED_0> <|Bot|>:é€‚åˆæ’å…¥å›¾åƒçš„è¡Œæ˜¯'

        for _ in progress.tqdm([1], desc="image spotting"):
            output_text = self.generate(input_text,
                                        random=False,
                                        beam=5,
                                        max_length=300,
                                        repetition=1.)
        inject_text = 'é€‚åˆæ’å…¥å›¾åƒçš„è¡Œæ˜¯' + output_text
        print(inject_text)

        locs = []
        for m in self.r2.findall(inject_text):
            locs.append(int(m[4:-1]))
        print(locs)
        return inject_text, locs

    def generate_cap(self, text_sections, pos, progress):
        pasts = ''
        caps = {}
        for idx, po in progress.tqdm(enumerate(pos), desc="image captioning"):
            full_txt = ''.join(text_sections[:po + 2])
            if idx > 0:
                past = pasts[:-2] + 'ã€‚'
            else:
                past = pasts

            input_text = f' <|User|>: ç»™å®šæ–‡ç« "{full_txt}" {past}ç»™å‡ºé€‚åˆåœ¨<Seg{po}>åæ’å…¥çš„å›¾åƒå¯¹åº”çš„æ ‡é¢˜ã€‚' + ' \n<TOKENS_UNUSED_0> <|Bot|>: æ ‡é¢˜æ˜¯"'

            cap_text = self.generate(input_text,
                                     random=False,
                                     beam=1,
                                     max_length=100,
                                     repetition=5.)
            cap_text = cap_text.split('"')[0].strip()
            print(cap_text)
            caps[po] = cap_text

            if idx == 0:
                pasts = f'ç°åœ¨<Seg{po}>åæ’å…¥å›¾åƒå¯¹åº”çš„æ ‡é¢˜æ˜¯"{cap_text}"ï¼Œ '
            else:
                pasts += f'<Seg{po}>åæ’å…¥å›¾åƒå¯¹åº”çš„æ ‡é¢˜æ˜¯"{cap_text}"ï¼Œ '

        print(caps)
        return caps

    def generate_loc_cap(self, text_sections, image_num, progress):
        inject_text, locs = self.generate_loc(text_sections, image_num,
                                              progress)
        caps = self.generate_cap(text_sections, locs, progress)
        return caps

    def interleav_wrap(self, img_embeds, text):
        batch_size = img_embeds.shape[0]
        im_len = img_embeds.shape[1]
        text = text[0]
        text = text.replace('<Img>', '')
        text = text.replace('</Img>', '')
        parts = text.split('<ImageHere>')
        assert batch_size + 1 == len(parts)
        warp_tokens = []
        warp_embeds = []
        warp_attns = []
        soi = (torch.ones([1, 1]) * self.soi_id).long().to(img_embeds.device)
        soi_embeds = self.llm_model.internlm_model.model.embed_tokens(soi)
        temp_len = 0

        for idx, part in enumerate(parts):
            if len(part) > 0:
                part_tokens = self.llm_model.internlm_tokenizer(
                    part, return_tensors="pt",
                    add_special_tokens=False).to(img_embeds.device)
                part_embeds = self.llm_model.internlm_model.model.embed_tokens(
                    part_tokens.input_ids)

                warp_tokens.append(part_tokens.input_ids)
                warp_embeds.append(part_embeds)
                temp_len += part_embeds.shape[1]
            if idx < batch_size:
                warp_tokens.append(soi.expand(-1, img_embeds[idx].shape[0]))
                # warp_tokens.append(soi.expand(-1, img_embeds[idx].shape[0] + 1))
                # warp_embeds.append(soi_embeds) ### 1, 1, C
                warp_embeds.append(img_embeds[idx].unsqueeze(0))  ### 1, 34, C
                temp_len += im_len

            if temp_len > self.max_txt_len:
                break

        warp_embeds = torch.cat(warp_embeds, dim=1)

        return warp_embeds[:, :self.max_txt_len].to(img_embeds.device)

    def align_text(self, samples):
        text_new = []
        text = [t + self.eoa + ' </s>' for t in samples["text_input"]]
        for i in range(len(text)):
            temp = text[i]
            temp = temp.replace('###Human', '<|User|>')
            temp = temp.replace('### Human', '<|User|>')
            temp = temp.replace('<|User|> :', '<|User|>:')
            temp = temp.replace('<|User|>: ', '<|User|>:')
            temp = temp.replace('<|User|>', ' <|User|>')

            temp = temp.replace('###Assistant', '<|Bot|>')
            temp = temp.replace('### Assistant', '<|Bot|>')
            temp = temp.replace('<|Bot|> :', '<|Bot|>:')
            temp = temp.replace('<|Bot|>: ', '<|Bot|>:')
            temp = temp.replace('<|Bot|>', self.eoh + ' <|Bot|>')
            if temp.find('<|User|>') > temp.find('<|Bot|>'):
                temp = temp.replace(' <|User|>', self.eoa + ' <|User|>')
            text_new.append(temp)
            #print (temp)
        return text_new

    def model_select_image(self, output_text, caps, root, progress):
        print('model_select_image')
        pre_text = ''
        pre_img = []
        pre_text_list = []
        ans2idx = {'A': 0, 'B': 1, 'C': 2, 'D': 3}
        selected = {k: 0 for k in caps.keys()}
        for i, text in enumerate(output_text.split('\n')):
            pre_text += text + '\n'
            if i in caps:
                images = copy.deepcopy(pre_img)
                for j in range(4):
                    image = Image.open(
                        os.path.join(
                            root, f'temp_{self.show_ids[i] * 1000 + i}_{j}.png'
                        )).convert("RGB")
                    image = self.vis_processor(image)
                    images.append(image)
                images = torch.stack(images, dim=0)

                pre_text_list.append(pre_text)
                pre_text = ''

                images = images.cuda()
                instruct = ' <|User|>:æ ¹æ®ç»™å®šä¸Šä¸‹æ–‡å’Œå€™é€‰å›¾åƒï¼Œé€‰æ‹©åˆé€‚çš„é…å›¾ï¼š'
                input_text = '<ImageHere>'.join(
                    pre_text_list
                ) + '\n\nå€™é€‰å›¾åƒåŒ…æ‹¬: A.<ImageHere>\nB.<ImageHere>\nC.<ImageHere>\nD.<ImageHere>\n\n<TOKENS_UNUSED_0> <|Bot|>:æœ€åˆé€‚çš„å›¾æ˜¯'
                input_text = instruct + input_text
                samples = {}
                samples['text_input'] = [input_text]
                self.llm_model.debug_flag = 0
                with torch.no_grad():
                    with torch.cuda.amp.autocast():
                        img_embeds = self.llm_model.encode_img(images)
                        input_text = self.align_text(samples)
                        img_embeds = self.interleav_wrap(
                            img_embeds, input_text)
                bos = torch.ones(
                    [1, 1]) * self.llm_model.internlm_tokenizer.bos_token_id
                bos = bos.long().to(images.device)
                meta_embeds = self.llm_model.internlm_model.model.embed_tokens(
                    bos)
                inputs_embeds = torch.cat([meta_embeds, img_embeds], dim=1)

                with torch.cuda.amp.autocast():
                    outputs = self.llm_model.internlm_model.generate(
                        inputs_embeds=inputs_embeds[:, :-2],
                        do_sample=False,
                        num_beams=5,
                        max_length=10,
                        repetition_penalty=1.,
                    )
                out_text = self.llm_model.internlm_tokenizer.decode(
                    outputs[0][1:], add_special_tokens=False)

                answer = out_text[1] if out_text[0] == ' ' else out_text[0]
                pre_img.append(images[len(pre_img) + ans2idx[answer]].cpu())
                selected[i] = ans2idx[answer]
        return selected

    def show_md(self, text_sections, title, caps, selected, show_cap=False):
        md_shows = []
        ga_shows = []
        btn_shows = []
        cap_textboxs, cap_searchs = [], []
        editers = []
        for i in range(len(text_sections)):
            if i in caps:
                if show_cap:
                    md = text_sections[
                        i] + '\n' + '<div align="center"> <img src="file/articles/{}/temp_{}_{}.png" width = 500/> {} </div>'.format(
                            title, self.show_ids[i] * 1000 + i, selected[i],
                            caps[i])
                else:
                    md = text_sections[
                        i] + '\n' + '<div align="center"> <img src="file=articles/{}/temp_{}_{}.png" width = 500/> </div>'.format(
                            title, self.show_ids[i] * 1000 + i, selected[i])
                img_list = [('articles/{}/temp_{}_{}.png'.format(
                    title, self.show_ids[i] * 1000 + i,
                    j), 'articles/{}/temp_{}_{}.png'.format(
                        title, self.show_ids[i] * 1000 + i, j))
                            for j in range(4)]

                ga_show = gr.Gallery.update(visible=True, value=img_list)
                ga_shows.append(ga_show)

                btn_show = gr.Button.update(visible=True,
                                            value='\U0001f5d1\uFE0F')

                cap_textboxs.append(
                    gr.Textbox.update(visible=True, value=caps[i]))
                cap_searchs.append(gr.Button.update(visible=True))
            else:
                md = text_sections[i]
                ga_show = gr.Gallery.update(visible=False, value=[])
                ga_shows.append(ga_show)

                btn_show = gr.Button.update(visible=True, value='\u2795')
                cap_textboxs.append(gr.Textbox.update(visible=False))
                cap_searchs.append(gr.Button.update(visible=False))

            md_show = gr.Markdown.update(visible=True, value=md)
            md_shows.append(md_show)
            btn_shows.append(btn_show)
            editers.append(gr.update(visible=True))
            print(i, md)

        md_hides = []
        ga_hides = []
        btn_hides = []
        for i in range(max_section - len(text_sections)):
            md_hide = gr.Markdown.update(visible=False, value='')
            md_hides.append(md_hide)

            btn_hide = gr.Button.update(visible=False)
            btn_hides.append(btn_hide)
            editers.append(gr.update(visible=False))

        for i in range(max_section - len(ga_shows)):
            ga_hide = gr.Gallery.update(visible=False, value=[])
            ga_hides.append(ga_hide)
            cap_textboxs.append(gr.Textbox.update(visible=False))
            cap_searchs.append(gr.Button.update(visible=False))

        return md_shows + md_hides + ga_shows + ga_hides + btn_shows + btn_hides + cap_textboxs + cap_searchs + editers, md_shows

    def generate_article(self,
                         title,
                         beam,
                         repetition,
                         text_num,
                         msi,
                         random,
                         progress=gr.Progress()):
        self.reset()
        self.title = title
        if article_stream_output:
            text = ' <|User|>:æ ¹æ®ç»™å®šæ ‡é¢˜å†™ä¸€ä¸ªå›¾æ–‡å¹¶èŒ‚ï¼Œä¸é‡å¤çš„æ–‡ç« ï¼š{}\n'.format(
                title) + self.eoh + ' <|Bot|>:'
            input_tokens = self.llm_model.internlm_tokenizer(
                text, return_tensors="pt",
                add_special_tokens=True).to(self.llm_model.device)
            img_embeds = self.llm_model.internlm_model.model.embed_tokens(
                input_tokens.input_ids)
            generate_params = dict(
                inputs_embeds=img_embeds,
                num_beams=beam,
                do_sample=random,
                stopping_criteria=self.stopping_criteria,
                repetition_penalty=float(repetition),
                max_length=text_num,
                bos_token_id=self.llm_model.internlm_tokenizer.bos_token_id,
                eos_token_id=self.llm_model.internlm_tokenizer.eos_token_id,
                pad_token_id=self.llm_model.internlm_tokenizer.pad_token_id,
            )
            output_text = "â–Œ"
            with self.generate_with_streaming(**generate_params) as generator:
                for output in generator:
                    decoded_output = self.llm_model.internlm_tokenizer.decode(
                        output[1:])
                    if output[-1] in [
                            self.llm_model.internlm_tokenizer.eos_token_id
                    ]:
                        break
                    output_text = decoded_output.replace('\n', '\n\n') + "â–Œ"
                    yield (output_text,) + (gr.Markdown.update(visible=False),) * (max_section - 1) + (gr.Gallery.update(visible=False),) * max_section + \
                          (gr.Button.update(visible=False),) * max_section + (gr.Textbox.update(visible=False),) * max_section + (gr.Button.update(visible=False),) * max_section + \
                          (gr.update(visible=False),) * max_section + (disable_btn,) * 2
                    time.sleep(0.03)
            output_text = output_text[:-1]
            yield (output_text,) + (gr.Markdown.update(visible=False),) * (max_section - 1) + (gr.Gallery.update(visible=False),) * max_section + \
                  (gr.Button.update(visible=False),) * max_section + (gr.Textbox.update(visible=False),) * max_section + (gr.Button.update(visible=False),) * max_section +\
                  (gr.update(visible=False),) * max_section + (disable_btn,) * 2
        else:
            output_text = self.generate_text(title, beam, repetition, text_num,
                                             random)

        print(output_text)
        output_text = re.sub(r'(\n[ \t]*)+', '\n', output_text)
        if output_text[-1] == '\n':
            output_text = output_text[:-1]
        print(output_text)
        output_text = '\n'.join(output_text.split('\n')[:max_section])

        text_sections = output_text.split('\n')
        idx_text_sections = [
            f'<Seg{i}>' + ' ' + it + '\n' for i, it in enumerate(text_sections)
        ]
        caps = self.generate_loc_cap(idx_text_sections, '', progress)
        #caps = {0: 'æˆéƒ½çš„ä¸‰æ—¥æ¸¸è·¯çº¿å›¾ï¼ŒåŒ…æ‹¬æ˜¥ç†™è·¯ã€å¤ªå¤é‡Œã€IFSå›½é‡‘ä¸­å¿ƒã€å¤§æ…ˆå¯ºã€å®½çª„å··å­ã€å¥æ˜Ÿæ¥¼è¡—ã€ä¹çœ¼æ¡¥ï¼ˆé…’å§ä¸€æ¡è¡—ï¼‰ã€æ­¦ä¾¯ç¥ ã€é”¦é‡Œã€æœç”«è‰å ‚ã€æµ£èŠ±æºªå…¬å›­ã€é’ç¾Šå®«ã€é‡‘æ²™é—å€åšç‰©é¦†ã€æ–‡æ®Šé™¢ã€äººæ°‘å…¬å›­ã€ç†ŠçŒ«åŸºåœ°ã€æœ›æ±Ÿæ¥¼å…¬å›­ã€ä¸œéƒŠè®°å¿†ã€å»ºè®¾è·¯å°åƒè¡—ã€ç”µå­ç§‘å¤§æ¸…æ°´æ²³æ ¡åŒºã€ä¸‰åœ£ä¹¡ä¸‡ç¦èŠ±å‰å¸‚åœºã€é¾™æ¹–æ»¨æ±Ÿå¤©è¡—è´­ç‰©å¹¿åœºå’Œè¿”ç¨‹ã€‚', 2: 'æ˜¥ç†™è·¯çš„ç¹åæ™¯è±¡ï¼Œå„ç§æ—¶å°šæ½®æµçš„å“ç‰Œåº—å’Œç¾é£Ÿé¤å…é³æ¬¡æ ‰æ¯”ã€‚', 4: 'IFSå›½é‡‘ä¸­å¿ƒçš„è±ªåè´­ç‰©ä¸­å¿ƒï¼Œæ‹¥æœ‰ä¼—å¤šå›½é™…çŸ¥åå“ç‰Œçš„æ——èˆ°åº—å’Œä¸“å–åº—ï¼ŒåŒæ—¶è¿˜æœ‰ç”µå½±é™¢ã€å¥èº«æˆ¿ é…å¥—è®¾æ–½ã€‚', 6: 'æ˜¥ç†™è·¯ä¸Šçš„è‘—åæ™¯ç‚¹â€”â€”å¤ªå¤é‡Œï¼Œæ˜¯ä¸€ä¸ªé›†è´­ç‰©ã€é¤é¥®ã€å¨±ä¹äºä¸€ä½“çš„é«˜ç«¯æ—¶å°šè¡—åŒºï¼Œä¹Ÿæ˜¯æˆéƒ½è‘—åçš„ç½‘çº¢æ‰“å¡åœ°ä¹‹ä¸€ã€‚', 8: 'å¤§æ…ˆå¯ºçš„å¤–è§‚ï¼Œæ˜¯ä¸€åº§å†å²æ‚ ä¹…çš„ä½›æ•™å¯ºåº™ï¼Œå§‹å»ºäºå”æœï¼Œæœ‰ç€æ·±åšçš„æ–‡åŒ–åº•è•´å’Œå†å²ä»·å€¼ã€‚'}
        #self.show_ids = {k:0 for k in caps.keys()}
        self.show_ids = {k: 1 for k in caps.keys()}

        print(caps)
        self.ex_idxs = []
        for loc, cap in progress.tqdm(caps.items(), desc="download image"):
            #self.show_ids[loc] += 1
            idxs = self.get_images_xlab(cap, loc, self.ex_idxs)
            self.ex_idxs.extend(idxs)

        if msi:
            self.selected = self.model_select_image(output_text, caps,
                                                    'articles/' + title,
                                                    progress)
        else:
            self.selected = {k: 0 for k in caps.keys()}
        components, md_shows = self.show_md(text_sections, title, caps,
                                            self.selected)
        self.show_caps = False

        self.output_text = output_text
        self.caps = caps
        if article_stream_output:
            yield components + [enable_btn] * 2
        else:
            return components + [enable_btn] * 2

    def adjust_img(self, img_num, progress=gr.Progress):
        text_sections = self.output_text.split('\n')
        idx_text_sections = [
            f'<Seg{i}>' + ' ' + it + '\n' for i, it in enumerate(text_sections)
        ]
        img_num = min(img_num, len(text_sections))
        caps = self.generate_loc_cap(idx_text_sections, int(img_num), progress)
        #caps = {1:'318å·è—çº¿æ²¿é€”çš„é£æ™¯ç…§ç‰‡', 4:'æ³¸å®šæ¡¥çš„å…¨æ™¯ç…§ç‰‡', 6:'æŠ˜å¤šå±±å­å£çš„å…¨æ™¯ç…§ç‰‡', 8:'ç¨»åŸäºšä¸æœºåœºçš„å…¨æ™¯ç…§ç‰‡', 10:'å§Šå¦¹æ¹–çš„å…¨æ™¯ç…§ç‰‡'}

        print(caps)
        sidxs = []
        for loc, cap in caps.items():
            if loc in self.show_ids:
                self.show_ids[loc] += 1
            else:
                self.show_ids[loc] = 1
            idxs = self.get_images_xlab(cap, loc, sidxs)
            sidxs.extend(idxs)
        self.sidxs = sidxs

        self.selected = {k: 0 for k in caps.keys()}
        components, md_shows = self.show_md(text_sections, self.title, caps,
                                            self.selected)

        self.caps = caps
        return components

    def add_delete_image(self, text, status, index):
        index = int(index)
        if status == '\U0001f5d1\uFE0F':
            if index in self.caps:
                self.caps.pop(index)
                self.selected.pop(index)
            md_show = gr.Markdown.update(value=text.split('\n')[0])
            gallery = gr.Gallery.update(visible=False, value=[])
            btn_show = gr.Button.update(value='\u2795')
            cap_textbox = gr.Textbox.update(visible=False)
            cap_search = gr.Button.update(visible=False)
        else:
            md_show = gr.Markdown.update()
            gallery = gr.Gallery.update(visible=True, value=[])
            btn_show = gr.Button.update(value='\U0001f5d1\uFE0F')
            cap_textbox = gr.Textbox.update(visible=True)
            cap_search = gr.Button.update(visible=True)

        return md_show, gallery, btn_show, cap_textbox, cap_search

    def search_image(self, text, index):
        index = int(index)
        if text == '':
            return gr.Gallery.update()

        if index in self.show_ids:
            self.show_ids[index] += 1
        else:
            self.show_ids[index] = 1
        self.caps[index] = text
        idxs = self.get_images_xlab(text, index, self.ex_idxs)
        self.ex_idxs.extend(idxs)

        img_list = [('articles/{}/temp_{}_{}.png'.format(
            self.title, self.show_ids[index] * 1000 + index,
            j), 'articles/{}/temp_{}_{}.png'.format(
                self.title, self.show_ids[index] * 1000 + index, j))
                    for j in range(4)]
        ga_show = gr.Gallery.update(visible=True, value=img_list)
        return ga_show

    def replace_image(self, article, index, evt: gr.SelectData):
        index = int(index)
        self.selected[index] = evt.index
        if '<div align="center">' in article:
            return re.sub(r'file=.*.png', 'file={}'.format(evt.value), article)
        else:
            return article + '\n' + '<div align="center"> <img src="file={}" width = 500/> </div>'.format(
                evt.value)

    def add_delete_caption(self):
        self.show_caps = False if self.show_caps else True
        text_sections = self.output_text.split('\n')
        components, _ = self.show_md(text_sections,
                                     self.title,
                                     self.caps,
                                     selected=self.selected,
                                     show_cap=self.show_caps)
        return components

    def save(self):
        folder = 'save_articles/' + self.title
        if os.path.exists(folder):
            for item in os.listdir(folder):
                os.remove(os.path.join(folder, item))
        os.makedirs(folder, exist_ok=True)

        save_text = ''
        count = 0
        if len(self.output_text) > 0:
            text_sections = self.output_text.split('\n')
            for i in range(len(text_sections)):
                if i in self.caps:
                    if self.show_caps:
                        md = text_sections[
                            i] + '\n' + '<div align="center"> <img src="temp_{}_{}.png" width = 500/> {} </div>'.format(
                                self.show_ids[i] * 1000 + i, self.selected[i],
                                self.caps[i])
                    else:
                        md = text_sections[
                            i] + '\n' + '<div align="center"> <img src="temp_{}_{}.png" width = 500/> </div>'.format(
                                self.show_ids[i] * 1000 + i, self.selected[i])
                    count += 1
                else:
                    md = text_sections[i]

                save_text += md + '\n\n'
            save_text = save_text[:-2]

        with open(os.path.join(folder, 'io.MD'), 'w') as f:
            f.writelines(save_text)

        for k in self.caps.keys():
            shutil.copy(
                os.path.join(
                    'articles', self.title,
                    f'temp_{self.show_ids[k] * 1000 + k}_{self.selected[k]}.png'
                ), folder)
        archived = shutil.make_archive(folder, 'zip', folder)
        return archived

    def get_context_emb(self, state, img_list):
        prompt = state.get_prompt()
        print(prompt)
        prompt_segs = prompt.split('<Img><ImageHere></Img>')

        assert len(prompt_segs) == len(
            img_list
        ) + 1, "Unmatched numbers of image placeholders and images."
        seg_tokens = [
            self.llm_model.internlm_tokenizer(seg,
                                              return_tensors="pt",
                                              add_special_tokens=i == 0).to(
                                                  self.device).input_ids
            for i, seg in enumerate(prompt_segs)
        ]
        seg_embs = [
            self.llm_model.internlm_model.model.embed_tokens(seg_t)
            for seg_t in seg_tokens
        ]
        mixed_embs = [
            emb for pair in zip(seg_embs[:-1], img_list) for emb in pair
        ] + [seg_embs[-1]]
        mixed_embs = torch.cat(mixed_embs, dim=1)
        return mixed_embs

    def chat_ask(self, state, img_list, text, image):
        print(1111)
        state.skip_next = False
        if len(text) <= 0 and image is None:
            state.skip_next = True
            return (state, img_list, state.to_gradio_chatbot(), "",
                    None) + (no_change_btn, ) * 2

        if image is not None:
            image_pt = self.vis_processor(image).unsqueeze(0).to(0)
            image_emb = self.llm_model.encode_img(image_pt)
            img_list.append(image_emb)

            state.append_message(state.roles[0],
                                 ["<Img><ImageHere></Img>", image])

        if len(state.messages) > 0 and state.messages[-1][0] == state.roles[
                0] and isinstance(state.messages[-1][1], list):
            #state.messages[-1][1] = ' '.join([state.messages[-1][1], text])
            state.messages[-1][1][0] = ' '.join(
                [state.messages[-1][1][0], text])
        else:
            state.append_message(state.roles[0], text)

        print(state.messages)

        state.append_message(state.roles[1], None)

        return (state, img_list, state.to_gradio_chatbot(), "",
                None) + (disable_btn, ) * 2

    def generate_with_callback(self, callback=None, **kwargs):
        kwargs.setdefault("stopping_criteria",
                          transformers.StoppingCriteriaList())
        kwargs["stopping_criteria"].append(Stream(callback_func=callback))
        with torch.no_grad():
            with self.llm_model.maybe_autocast():
                self.llm_model.internlm_model.generate(**kwargs)

    def generate_with_streaming(self, **kwargs):
        return Iteratorize(self.generate_with_callback, kwargs, callback=None)

    def chat_answer(self, state, img_list, max_output_tokens,
                    repetition_penalty, num_beams, do_sample):
        # text = 'å›¾ç‰‡ä¸­æ˜¯ä¸€å¹…æ²¹ç”»ï¼Œæç»˜äº†çº¢å†›é•¿å¾çš„åœºæ™¯ã€‚ç”»é¢ä¸­ï¼Œä¸€ç¾¤çº¢å†›æˆ˜å£«æ­£åœ¨ç©¿è¿‡ä¸€ç‰‡è‰åœ°ï¼Œä»–ä»¬èº«åçš„æ——å¸œåœ¨é£ä¸­é£˜æ‰¬ã€‚'
        # for i in range(len(text)):
        #     state.messages[-1][-1] = text[:i+1] + "â–Œ"
        #     yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 2
        # state.messages[-1][-1] = text[:i + 1]
        # yield (state, state.to_gradio_chatbot()) + (enable_btn, ) * 2
        # return

        if state.skip_next:
            return (state, state.to_gradio_chatbot()) + (no_change_btn, ) * 2

        embs = self.get_context_emb(state, img_list)
        if chat_stream_output:
            generate_params = dict(
                inputs_embeds=embs,
                num_beams=num_beams,
                do_sample=do_sample,
                stopping_criteria=self.stopping_criteria,
                repetition_penalty=float(repetition_penalty),
                max_length=max_output_tokens,
                bos_token_id=self.llm_model.internlm_tokenizer.bos_token_id,
                eos_token_id=self.llm_model.internlm_tokenizer.eos_token_id,
                pad_token_id=self.llm_model.internlm_tokenizer.pad_token_id,
            )
            state.messages[-1][-1] = "â–Œ"
            with self.generate_with_streaming(**generate_params) as generator:
                for output in generator:
                    decoded_output = self.llm_model.internlm_tokenizer.decode(
                        output[1:])
                    if output[-1] in [
                            self.llm_model.internlm_tokenizer.eos_token_id
                    ]:
                        break
                    state.messages[-1][-1] = decoded_output + "â–Œ"
                    yield (state,
                           state.to_gradio_chatbot()) + (disable_btn, ) * 2
                    time.sleep(0.03)
            state.messages[-1][-1] = state.messages[-1][-1][:-1]
            yield (state, state.to_gradio_chatbot()) + (enable_btn, ) * 2
            return
        else:
            outputs = self.llm_model.internlm_model.generate(
                inputs_embeds=embs,
                max_new_tokens=max_output_tokens,
                stopping_criteria=self.stopping_criteria,
                num_beams=num_beams,
                #temperature=float(temperature),
                do_sample=do_sample,
                repetition_penalty=float(repetition_penalty),
                bos_token_id=self.llm_model.internlm_tokenizer.bos_token_id,
                eos_token_id=self.llm_model.internlm_tokenizer.eos_token_id,
                pad_token_id=self.llm_model.internlm_tokenizer.pad_token_id,
            )

            output_token = outputs[0]
            if output_token[0] == 0:
                output_token = output_token[1:]
            output_text = self.llm_model.internlm_tokenizer.decode(
                output_token, add_special_tokens=False)
            print(output_text)
            output_text = output_text.split('<TOKENS_UNUSED_1>')[
                0]  # remove the stop sign '###'
            output_text = output_text.split('Assistant:')[-1].strip()
            output_text = output_text.replace("<s>", "")
            state.messages[-1][1] = output_text

            return (state, state.to_gradio_chatbot()) + (enable_btn, ) * 2

    def clear_answer(self, state):
        state.messages[-1][-1] = None
        return (state, state.to_gradio_chatbot())

    def chat_clear_history(self):
        state = CONV_VISION_7132_v2.copy()
        return (state, [], state.to_gradio_chatbot(), "",
                None) + (disable_btn, ) * 2


def load_demo():
    state = CONV_VISION_7132_v2.copy()

    return (state, [], gr.Chatbot.update(visible=True),
            gr.Textbox.update(visible=True), gr.Button.update(visible=True),
            gr.Row.update(visible=True), gr.Accordion.update(visible=True))


def change_language(lang):
    if lang == 'ä¸­æ–‡':
        lang_btn = gr.update(value='English')
        title = gr.update(label='æ ¹æ®ç»™å®šæ ‡é¢˜å†™ä¸€ä¸ªå›¾æ–‡å¹¶èŒ‚çš„æ–‡ç« ï¼š')
        btn = gr.update(value='ç”Ÿæˆ')
        parameter_article = gr.update(label='é«˜çº§è®¾ç½®')

        beam = gr.update(label='é›†æŸå¤§å°')
        repetition = gr.update(label='é‡å¤æƒ©ç½š')
        text_num = gr.update(label='æœ€å¤šè¾“å‡ºå­—æ•°')
        msi = gr.update(label='æ¨¡å‹é€‰å›¾')
        random = gr.update(label='é‡‡æ ·')
        img_num = gr.update(label='ç”Ÿæˆæ–‡ç« åï¼Œå¯é€‰æ‹©å…¨æ–‡é…å›¾æ•°é‡')
        adjust_btn = gr.update(value='å›ºå®šæ•°é‡é…å›¾')
        cap_searchs, editers = [], []
        for _ in range(max_section):
            cap_searchs.append(gr.update(value='æœç´¢'))
            editers.append(gr.update(label='ç¼–è¾‘'))

        save_btn = gr.update(value='æ–‡ç« ä¸‹è½½')
        save_file = gr.update(label='æ–‡ç« ä¸‹è½½')

        parameter_chat = gr.update(label='å‚æ•°')
        chat_text_num = gr.update(label='æœ€å¤šè¾“å‡ºå­—æ•°')
        chat_beam = gr.update(label='é›†æŸå¤§å°')
        chat_repetition = gr.update(label='é‡å¤æƒ©ç½š')
        chat_random = gr.update(label='é‡‡æ ·')

        chat_textbox = gr.update(placeholder='è¾“å…¥èŠå¤©å†…å®¹å¹¶å›è½¦')
        submit_btn = gr.update(value='æäº¤')
        regenerate_btn = gr.update(value='ğŸ”„  é‡æ–°ç”Ÿæˆ')
        clear_btn = gr.update(value='ğŸ—‘ï¸  æ¸…ç©ºèŠå¤©æ¡†')
    elif lang == 'English':
        lang_btn = gr.update(value='ä¸­æ–‡')
        title = gr.update(
            label='Write an illustrated article based on the given title:')
        btn = gr.update(value='Submit')
        parameter_article = gr.update(label='Advanced Settings')

        beam = gr.update(label='Beam Size')
        repetition = gr.update(label='Repetition_penalty')
        text_num = gr.update(label='Max output tokens')
        msi = gr.update(label='Model selects images')
        random = gr.update(label='Do_sample')
        img_num = gr.update(
            label=
            'Select the number of the inserted image after article generation.'
        )
        adjust_btn = gr.update(value='Insert a fixed number of images')
        cap_searchs, editers = [], []
        for _ in range(max_section):
            cap_searchs.append(gr.update(value='Search'))
            editers.append(gr.update(label='edit'))

        save_btn = gr.update(value='Save article')
        save_file = gr.update(label='Save article')

        parameter_chat = gr.update(label='Parameters')
        chat_text_num = gr.update(label='Max output tokens')
        chat_beam = gr.update(label='Beam Size')
        chat_repetition = gr.update(label='Repetition_penalty')
        chat_random = gr.update(label='Do_sample')

        chat_textbox = gr.update(placeholder='Enter text and press ENTER')
        submit_btn = gr.update(value='Submit')
        regenerate_btn = gr.update(value='ğŸ”„  Regenerate')
        clear_btn = gr.update(value='ğŸ—‘ï¸  Clear history')

    return [lang_btn, title, btn, parameter_article, beam, repetition, text_num, msi, random, img_num, adjust_btn] +\
           cap_searchs + editers + [save_btn, save_file] +[parameter_chat, chat_text_num, chat_beam, chat_repetition, chat_random] + \
           [chat_textbox, submit_btn, regenerate_btn, clear_btn]


demo_ui = Demo_UI()

with gr.Blocks(css=custom_css, title='æµ¦è¯­Â·çµç¬” (InternLM-XComposer)') as demo:
    with gr.Row():
        with gr.Column(scale=20):
            #gr.HTML("""<h1 align="center" id="space-title" style="font-size:35px;">ğŸ¤— æµ¦è¯­Â·çµç¬” (InternLM-XComposer)</h1>""")
            gr.HTML(
                """<h1 align="center"><img src="https://raw.githubusercontent.com/panzhang0212/interleaved_io/main/logo.png", alt="InternLM-XComposer" border="0" style="margin: 0 auto; height: 200px;" /></a> </h1>"""
            )
        with gr.Column(scale=1, min_width=100):
            lang_btn = gr.Button("ä¸­æ–‡")

    with gr.Tabs(elem_classes="tab-buttons") as tabs:
        with gr.TabItem("ğŸ“ åˆ›ä½œå›¾æ–‡å¹¶èŒ‚æ–‡ç«  (Write Interleaved-text-image Article)"):
            with gr.Row():
                title = gr.Textbox(
                    label=
                    'Write an illustrated article based on the given title:',
                    scale=2)
                btn = gr.Button("Submit", scale=1)

            with gr.Row():
                img_num = gr.Slider(
                    minimum=1.0,
                    maximum=30.0,
                    value=5.0,
                    step=1.0,
                    scale=2,
                    label=
                    'Select the number of the inserted image after article generation.'
                )
                adjust_btn = gr.Button('Insert a fixed number of images',
                                       interactive=False,
                                       scale=1)

            with gr.Row():
                with gr.Column(scale=1):
                    with gr.Accordion("Advanced Settings",
                                      open=False,
                                      visible=True) as parameter_article:
                        beam = gr.Slider(minimum=1.0,
                                         maximum=6.0,
                                         value=5.0,
                                         step=1.0,
                                         label='Beam Size')
                        repetition = gr.Slider(minimum=0.0,
                                               maximum=10.0,
                                               value=5.0,
                                               step=0.1,
                                               label='Repetition_penalty')
                        text_num = gr.Slider(minimum=100.0,
                                             maximum=2000.0,
                                             value=1000.0,
                                             step=1.0,
                                             label='Max output tokens')
                        msi = gr.Checkbox(value=True,
                                          label='Model selects images')
                        random = gr.Checkbox(label='Do_sample')

                with gr.Column(scale=1):
                    gr.Examples(
                        examples=[["åˆè§æ•¦ç…Œ"], ["æ˜Ÿé“¾æ–°é—»ç¨¿"], ["å¦‚ä½•å…»å¥½ä¸€åªå® ç‰©"],
                                  ["Shanghai Travel Guide in English"], ["Travel guidance of London in English"], ["Advertising for Genshin Impact in English"]],
                        inputs=[title],
                    )

            articles = []
            gallerys = []
            add_delete_btns = []
            cap_textboxs = []
            cap_searchs = []
            editers = []
            with gr.Column():
                for i in range(max_section):
                    with gr.Row():
                        visible = True if i == 0 else False
                        with gr.Column(scale=2):
                            article = gr.Markdown(visible=visible,
                                                  elem_classes='feedback')
                            articles.append(article)

                        with gr.Column(scale=1):
                            with gr.Accordion('edit',
                                              open=False,
                                              visible=False) as editer:
                                with gr.Row():
                                    cap_textbox = gr.Textbox(show_label=False,
                                                             interactive=True,
                                                             scale=6,
                                                             visible=False)
                                    cap_search = gr.Button(value="Search",
                                                           visible=False,
                                                           scale=1)
                                with gr.Row():
                                    gallery = gr.Gallery(visible=False,
                                                         columns=2,
                                                         height='auto')

                                add_delete_btn = gr.Button(visible=False)

                            gallery.select(demo_ui.replace_image, [
                                articles[i],
                                gr.Number(value=i, visible=False)
                            ], articles[i])
                            gallerys.append(gallery)
                            add_delete_btns.append(add_delete_btn)

                            cap_textboxs.append(cap_textbox)
                            cap_searchs.append(cap_search)
                            editers.append(editer)

                save_btn = gr.Button("Save article")
                save_file = gr.File(label="Save article")

                for i in range(max_section):
                    add_delete_btns[i].click(demo_ui.add_delete_image,
                                             inputs=[
                                                 articles[i],
                                                 add_delete_btns[i],
                                                 gr.Number(value=i,
                                                           visible=False)
                                             ],
                                             outputs=[
                                                 articles[i], gallerys[i],
                                                 add_delete_btns[i],
                                                 cap_textboxs[i],
                                                 cap_searchs[i]
                                             ])
                    cap_searchs[i].click(demo_ui.search_image,
                                         inputs=[
                                             cap_textboxs[i],
                                             gr.Number(value=i, visible=False)
                                         ],
                                         outputs=gallerys[i])

                btn.click(
                    demo_ui.generate_article,
                    inputs=[title, beam, repetition, text_num, msi, random],
                    outputs=articles + gallerys + add_delete_btns +
                    cap_textboxs + cap_searchs + editers + [btn, adjust_btn])
                # cap_btn.click(demo_ui.add_delete_caption, inputs=None, outputs=articles)
                save_btn.click(demo_ui.save, inputs=None, outputs=save_file)
                adjust_btn.click(demo_ui.adjust_img,
                                 inputs=img_num,
                                 outputs=articles + gallerys +
                                 add_delete_btns + cap_textboxs + cap_searchs +
                                 editers)

        with gr.TabItem("ğŸ’¬ å¤šæ¨¡æ€å¯¹è¯ (Multimodal Chat)", elem_id="chat", id=0):
            chat_state = gr.State()
            img_list = gr.State()
            with gr.Row():
                with gr.Column(scale=3):
                    imagebox = gr.Image(type="pil")

                    with gr.Accordion("Parameters", open=True,
                                      visible=False) as parameter_row:
                        chat_max_output_tokens = gr.Slider(
                            minimum=0,
                            maximum=1024,
                            value=512,
                            step=64,
                            interactive=True,
                            label="Max output tokens",
                        )
                        chat_num_beams = gr.Slider(
                            minimum=1,
                            maximum=5,
                            value=3,
                            step=1,
                            interactive=True,
                            label="Beam Size",
                        )
                        chat_repetition_penalty = gr.Slider(
                            minimum=1,
                            maximum=5,
                            value=1,
                            step=0.1,
                            interactive=True,
                            label="Repetition_penalty",
                        )
                        # chat_temperature = gr.Slider(minimum=0, maximum=1, value=1, step=0.1, interactive=True,
                        #                         label="Temperature", )
                        chat_do_sample = gr.Checkbox(interactive=True,
                                                     value=True,
                                                     label="Do_sample")

                with gr.Column(scale=6):
                    chatbot = grChatbot(elem_id="chatbot",
                                        visible=False,
                                        height=750)
                    with gr.Row():
                        with gr.Column(scale=8):
                            chat_textbox = gr.Textbox(
                                show_label=False,
                                placeholder="Enter text and press ENTER",
                                visible=False).style(container=False)
                        with gr.Column(scale=1, min_width=60):
                            submit_btn = gr.Button(value="Submit",
                                                   visible=False)
                    with gr.Row(visible=True) as button_row:
                        regenerate_btn = gr.Button(value="ğŸ”„  Regenerate",
                                                   interactive=False)
                        clear_btn = gr.Button(value="ğŸ—‘ï¸  Clear history",
                                              interactive=False)

            btn_list = [regenerate_btn, clear_btn]
            parameter_list = [
                chat_max_output_tokens, chat_repetition_penalty,
                chat_num_beams, chat_do_sample
            ]

            chat_textbox.submit(
                demo_ui.chat_ask,
                [chat_state, img_list, chat_textbox, imagebox],
                [chat_state, img_list, chatbot, chat_textbox, imagebox] +
                btn_list).then(demo_ui.chat_answer,
                               [chat_state, img_list] + parameter_list,
                               [chat_state, chatbot] + btn_list)
            submit_btn.click(
                demo_ui.chat_ask,
                [chat_state, img_list, chat_textbox, imagebox],
                [chat_state, img_list, chatbot, chat_textbox, imagebox] +
                btn_list).then(demo_ui.chat_answer,
                               [chat_state, img_list] + parameter_list,
                               [chat_state, chatbot] + btn_list)

            regenerate_btn.click(demo_ui.clear_answer, chat_state,
                                 [chat_state, chatbot]).then(
                                     demo_ui.chat_answer,
                                     [chat_state, img_list] + parameter_list,
                                     [chat_state, chatbot] + btn_list)
            clear_btn.click(
                demo_ui.chat_clear_history, None,
                [chat_state, img_list, chatbot, chat_textbox, imagebox] +
                btn_list)

            demo.load(load_demo, None, [
                chat_state, img_list, chatbot, chat_textbox, submit_btn,
                parameter_row
            ])

    lang_btn.click(change_language, inputs=lang_btn, outputs=[lang_btn, title, btn, parameter_article] +\
                                [beam, repetition, text_num, msi, random, img_num, adjust_btn] + cap_searchs + editers +\
                                [save_btn, save_file] + [parameter_row, chat_max_output_tokens, chat_num_beams, chat_repetition_penalty, chat_do_sample] +\
                                [chat_textbox, submit_btn, regenerate_btn, clear_btn])
    demo.queue(concurrency_count=8, status_update_rate=10, api_open=False)

if __name__ == "__main__":
    demo.launch(share=True, server_name="0.0.0.0", server_port=10111)
