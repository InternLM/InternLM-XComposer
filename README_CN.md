<p align="center">
    <img src="./assets/logo_cn.png" width="400"/>
</p>
<p align="center">
    <b><font size="6">æµ¦è¯­Â·çµç¬”2</font></b>
</p>

<!-- <div align="center">
        InternLM-XComposer <a href="">ğŸ¤– <a> <a href="">ğŸ¤—</a>&nbsp ï½œ InternLM-VL <a href="">ğŸ¤– <a> <a href="">ğŸ¤—</a>&nbsp | Technical Report <a href=""> <a> ğŸ“„  -->

<div align="center">
        InternLM-XComposer2 <a href="https://huggingface.co/internlm/internlm-xcomposer2-7b">ğŸ¤—</a> <a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-7b"><img src="./assets/modelscope_logo.png" width="20px"></a> &nbspï½œ InternLM-XComposer2-VL <a href="https://huggingface.co/internlm/internlm-xcomposer2-vl-7b">ğŸ¤—</a> <a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-vl-7b"><img src="./assets/modelscope_logo.png" width="20px"></a> &nbsp | æŠ€æœ¯æŠ¥å‘Š <a href="">  ğŸ“„ </a>

[English](./README.md) | [ç®€ä½“ä¸­æ–‡](./README_CN.md)


<p align="center">
    æ„Ÿè°¢ç¤¾åŒºæä¾›çš„ InternLM-XComposer2 <a href="https://huggingface.co/spaces/Willow123/InternLM-XComposer">åœ¨çº¿è¯•ç”¨</a>
</p>

</div>
<p align="center">
    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> å’Œ <a href="https://github.com/InternLM/InternLM/assets/25839884/a6aad896-7232-4220-ac84-9e070c2633ce" target="_blank">å¾®ä¿¡ç¤¾åŒº</a>
</p>

<br>

## æœ¬ä»“åº“åŒ…æ‹¬çš„å¤šæ¨¡æ€é¡¹ç›®

> [**InternLM-XComposer2**](https://github.com/InternLM/InternLM-XComposer): **Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Models**

> [**InternLM-XComposer**](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-1.0): **A Vision-Language Large Model for Advanced Text-image Comprehension and Composition**

> <img src="https://raw.githubusercontent.com/ShareGPT4V/ShareGPT4V-Resources/master/images/logo_tight.png" style="vertical-align: -20px;" :height="25px" width="25px">[**ShareGPT4V**](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V): **Improving Large Multi-modal Models with Better Captions**

</br>



**æµ¦è¯­Â·çµç¬”2**æ˜¯åŸºäº[ä¹¦ç”ŸÂ·æµ¦è¯­2](https://github.com/InternLM/InternLM/tree/main)å¤§è¯­è¨€æ¨¡å‹ç ”å‘çš„çªç ´æ€§çš„å›¾æ–‡å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œå…·æœ‰éå‡¡çš„å›¾æ–‡å†™ä½œå’Œå›¾åƒç†è§£èƒ½åŠ›ï¼Œåœ¨å¤šç§åº”ç”¨åœºæ™¯è¡¨ç°å‡ºè‰²ï¼š

- **è‡ªç”±æŒ‡ä»¤è¾“å…¥çš„å›¾æ–‡å†™ä½œï¼š** æµ¦è¯­Â·çµç¬”2å¯ä»¥ç†è§£**è‡ªç”±å½¢å¼çš„å›¾æ–‡æŒ‡ä»¤è¾“å…¥ï¼ŒåŒ…æ‹¬å¤§çº²ã€æ–‡ç« ç»†èŠ‚è¦æ±‚ã€å‚è€ƒå›¾ç‰‡ç­‰**ï¼Œä¸ºç”¨æˆ·æ‰“é€ å›¾æ–‡å¹¶è²Œçš„ä¸“å±æ–‡ç« ã€‚ç”Ÿæˆçš„æ–‡ç« æ–‡é‡‡æ–ç„¶ï¼Œå›¾æ–‡ç›¸å¾—ç›Šå½°ï¼Œæä¾›æ²‰æµ¸å¼çš„é˜…è¯»ä½“éªŒã€‚

- **å‡†ç¡®çš„å›¾æ–‡é—®é¢˜è§£ç­”ï¼š** æµ¦è¯­Â·çµç¬”2å…·æœ‰æµ·é‡å›¾æ–‡çŸ¥è¯†ï¼Œå¯ä»¥å‡†ç¡®çš„å›å¤å„ç§å›¾æ–‡é—®ç­”éš¾é¢˜ï¼Œåœ¨è¯†åˆ«ã€æ„ŸçŸ¥ã€ç»†èŠ‚æè¿°ã€è§†è§‰æ¨ç†ç­‰èƒ½åŠ›ä¸Šè¡¨ç°æƒŠäººã€‚

- **æ°å‡ºæ€§èƒ½ï¼š** æµ¦è¯­Â·çµç¬”2åŸºäºä¹¦ç”ŸÂ·æµ¦è¯­2-7Bæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨13é¡¹å¤šæ¨¡æ€è¯„æµ‹ä¸­å¤§å¹…é¢†å…ˆåŒé‡çº§å¤šæ¨¡æ€æ¨¡å‹ï¼Œåœ¨å…¶ä¸­6é¡¹è¯„æµ‹ä¸­è¶…è¿‡ GPT-4V å’Œ Gemini Proã€‚

<p align="center">
    <img src="assets/benchmark.png" width="1000"/>
</p>

æˆ‘ä»¬å¼€æºçš„ æµ¦è¯­Â·çµç¬”2 åŒ…æ‹¬ä¸¤ä¸ªç‰ˆæœ¬:

- **InternLM-XComposer2-VL-7B** <a href="https://huggingface.co/internlm/internlm-xcomposer2-vl-7b">ğŸ¤—</a> <a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-vl-7b"><img src="./assets/modelscope_logo.png" width="20px"> </a>ï¼ˆæµ¦è¯­Â·çµç¬”2-è§†è§‰é—®ç­”-7Bï¼‰: åŸºäºä¹¦ç”ŸÂ·æµ¦è¯­2-7Bå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒï¼Œé¢å‘å¤šæ¨¡æ€è¯„æµ‹å’Œè§†è§‰é—®ç­”ã€‚æµ¦è¯­Â·çµç¬”2-è§†è§‰é—®ç­”-7Bæ˜¯ç›®å‰æœ€å¼ºçš„åŸºäº7Bé‡çº§è¯­è¨€æ¨¡å‹åŸºåº§çš„å›¾æ–‡å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œé¢†è·‘å¤šè¾¾13ä¸ªå¤šæ¨¡æ€å¤§æ¨¡å‹æ¦œå•ã€‚

- **InternLM-XComposer2-7B** <a href="https://huggingface.co/internlm/internlm-xcomposer2-vl-7b">ğŸ¤—</a> <a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-vl2-7b"><img src="./assets/modelscope_logo.png" width="20px"> </a>: è¿›ä¸€æ­¥å¾®è°ƒï¼Œæ”¯æŒè‡ªç”±æŒ‡ä»¤è¾“å…¥å›¾æ–‡å†™ä½œçš„å›¾æ–‡å¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚
 
æ›´å¤šæ–¹æ³•ç»†èŠ‚è¯·å‚è€ƒ[æŠ€æœ¯æŠ¥å‘Š]()ï¼
  <br>

<!-- 
<p align="center">
    <figcaption align = "center"><b> InternLM-XComposer </b></figcaption>
<p> -->


<!-- ## Demo



https://github.com/InternLM/InternLM-XComposer/assets/22662425/0a2b475b-3f74-4f41-a5df-796680fa56cd
 -->

## Demo Video

<video src='https://openxlab.oss-cn-shanghai.aliyuncs.com/xcomposer-writer/InternLM_XComposer_demo_CN.mp4' controls='' height=540 width=960>
</video>



## æ›´æ–°æ¶ˆæ¯
* ```2023.01.26``` ğŸ‰ğŸ‰ğŸ‰ **InternLM-XComposer-VL-7B**çš„[è¯„æµ‹ä»£ç ](./evaluation/)å·²å¼€æºã€‚
* ```2023.01.26``` ğŸ‰ğŸ‰ğŸ‰ [InternLM-XComposer2-7B](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-7b) and [InternLM-XComposer-VL2-7B](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-vl-7b)å·²åœ¨**ModelScope**å¼€æºã€‚
* ```2023.01.26``` ğŸ‰ğŸ‰ğŸ‰ [InternLM-XComposer2-7B](https://huggingface.co/internlm/internlm-xcomposer2-7b) and [InternLM-XComposer-VL2-7B](https://huggingface.co/internlm/internlm-xcomposer2-vl-7b)å·²åœ¨**Hugging Face**å¼€æºã€‚
* ```2023.01.26``` ğŸ‰ğŸ‰ğŸ‰ æˆ‘ä»¬å…¬å¼€äº†InternLM-XComposer2æ›´å¤šæŠ€æœ¯ç»†èŠ‚ï¼Œè¯·å‚è€ƒ[æŠ€æœ¯æŠ¥å‘Š]()ã€‚
* ```2023.11.22``` ğŸ‰ğŸ‰ğŸ‰ æˆ‘ä»¬å¼€æºäº†[ShareGPT4V](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V), ä¸€ä¸ªé«˜è´¨é‡çš„å¤§è§„æ¨¡å›¾æ–‡æè¿°æ•°æ®é›†ï¼Œä»¥åŠæ€§èƒ½ä¼˜ç§€çš„å¤šæ¨¡æ€å¤§æ¨¡å‹ShareGPT4V-7Bã€‚
* ```2023.10.30``` ğŸ‰ğŸ‰ğŸ‰ çµç¬”åœ¨[Q-Bench](https://github.com/Q-Future/Q-Bench/tree/master/leaderboards#overall-leaderboards) å’Œ [Tiny LVLM](https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/tiny_lvlm_evaluation) å–å¾—äº†ç¬¬ä¸€åã€‚
* ```2023.10.19``` ğŸ‰ğŸ‰ğŸ‰ æ”¯æŒå¤šå¡æµ‹è¯•ï¼Œå¤šå¡Demo. ä¸¤å¼ 4090æ˜¾å¡å¯éƒ¨ç½²å…¨é‡Demoã€‚
* ```2023.10.12``` ğŸ‰ğŸ‰ğŸ‰ æ”¯æŒ4æ¯”ç‰¹é‡åŒ–Demoï¼Œ æ¨¡å‹æ–‡ä»¶å¯ä»[Hugging Face](https://huggingface.co/internlm/internlm-xcomposer-7b-4bit) and [ModelScope](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-7b-4bit) è·å–ã€‚
* ```2023.10.8``` ğŸ‰ğŸ‰ğŸ‰ [InternLM-XComposer-7B](https://huggingface.co/internlm/internlm-xcomposer-7b) å’Œ [InternLM-XComposer-VL-7B](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-vl-7b) å·²åœ¨Modelscopeå¼€æºã€‚
* ```2023.9.27``` ğŸ‰ğŸ‰ğŸ‰ **InternLM-XComposer-VL-7B**çš„[è¯„æµ‹ä»£ç ](./evaluation/)å·²å¼€æºã€‚
* ```2023.9.27``` ğŸ‰ğŸ‰ğŸ‰ [InternLM-XComposer-7B](https://huggingface.co/internlm/internlm-xcomposer-7b) å’Œ [InternLM-XComposer-VL-7B](https://huggingface.co/internlm/internlm-xcomposer-vl-7b) å·²åœ¨Hugging Faceå¼€æºã€‚
* ```2023.9.27``` ğŸ‰ğŸ‰ğŸ‰ æ›´å¤šæŠ€æœ¯ç»†èŠ‚è¯·å‚è€ƒ[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/pdf/2309.15112.pdf)ã€‚
</br>


## è¯„æµ‹

æˆ‘ä»¬åœ¨13ä¸ªå¤šæ¨¡æ€è¯„æµ‹å¯¹InternLM-XComposer2-VLä¸Šè¿›è¡Œæµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š[MathVista](https://mathvista.github.io/), [MMMU](https://mmmu-benchmark.github.io/), [AI2D](https://prior.allenai.org/projects/diagram-understanding), [MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation), [MMBench](https://opencompass.org.cn/leaderboard-multimodal), [MMBench-CN](https://opencompass.org.cn/leaderboard-multimodal), [SEED-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard), [QBench](https://github.com/Q-Future/Q-Bench/tree/master/leaderboards#overall-leaderboards), [HallusionBench](https://github.com/tianyi-lab/HallusionBench), [ChartQA](https://github.com/vis-nlp/ChartQA), [MM-Vet](https://github.com/yuweihao/MM-Vet), [LLaVA-in-the-wild](https://github.com/haotian-liu/LLaVA), [POPE](https://github.com/AoiDragon/POPE).


å¤ç°è¯„æµ‹ç»“æœï¼Œè¯·å‚è€ƒ[è¯„æµ‹ç»†èŠ‚](./evaluation/README.md)ã€‚
 


### å¯¹æ¯”é—­æºå¤šæ¨¡æ€APIä»¥åŠå¼€æºSOTAæ¨¡å‹ã€‚
|               | MathVista | AI2D   | MMMU  | MME    | MMB    | MMBCN  | SEEDI | LLaVAW | QBenchT | MM-Vet | HallB  | ChartVQA  |
|---------------|-----------|--------|-------|--------|--------|--------|-------|--------|---------|--------|--------|-----------|
|  Open-source Previous  SOTA | SPH-MOE   | Monkey | Yi-VL | WeMM   | L-Int2 | L-Int2 | SPH-2 | CogVLM | Int-XC  | CogVLM | Monkey | CogAgent  |
|    | 8x7B      | 10B    | 34B   | 6B     | 20B    | 20B    | 17B   | 17B    | 8B      | 30B    | 10B    | 18B       |
|  | 42.3      | 72.6   | 45.9  | 2066.6 | 75.1   | 73.7   | 74.8  | 73.9   | 64.4    | 56.8   | 58.4   | 68.4      |
|               |           |        |       |        |        |        |       |        |         |        |        |           |
| GPT-4V        | 49.9      | 78.2   | 56.8  | 1926.5 | 77     | 74.4   | 69.1  | 93.1   | 74.1    | 67.7   | 65.8   | 78.5      |
| Gemini-Pro    | 45.2      | 73.9   | 47.9  | 1933.3 | 73.6   | 74.3   | 70.7  | 79.9   | 70.6    | 64.3   | 63.9   | 74.1      |
| QwenVL-Plus   | 43.3      | 75.9   | 46.5  | 2183.3 | 67     | 70.7   | 72.7  | 73.7   | 68.9    | 55.7   | 56.4   | 78.1      |
| Ours          | 57.6      | 78.7   | 42    | 2242.7 | 79.6   | 77.6   | 75.9  | 81.8   | 72.5    | 51.2   | 60.3   | 72.6      |


### å¯¹æ¯”å¼€æºæ¨¡å‹ã€‚
| Method       | LLM          | MathVista | MMMU | MMEP     | MMEC  | MMB  | MMBCN | SEEDI | LLaVAW | QBenchT | MM-Vet | HallB  | POPE  |
|--------------|--------------|-----------|------|----------|-------|------|-------|-------|--------|---------|--------|--------|--------|
| BLIP-2       | FLAN-T5      | -         | 35.7 | 1,293.8  | 290.0 | -    | -     | 46.4  | 38.1   | -       | 22.4   | -      | -      |
| InstructBLIP | Vicuna-7B    | 25.3      | 30.6 | -        | -     | 36.0 | 23.7  | 53.4  | 60.9   | 55.9    | 26.2   | 53.6   | 78.9   |
| IDEFICS-80B  | LLaMA-65B    | 26.2      | 24.0 | -        | -     | 54.5 | 38.1  | 52.0  | 56.9   | -       | 39.7   | 46.1   | -      |
| Qwen-VL-Chat | Qwen-7B      | 33.8      | 35.9 | 1,487.5  | 360.7 | 60.6 | 56.7  | 58.2  | 67.7   | 61.7    | 47.3   | 56.4   | -      |
| LLaVA        | Vicuna-7B    | 23.7      | 32.3 | 807.0    | 247.9 | 34.1 | 14.1  | 25.5  | 63.0   | 54.7    | 26.7   | 44.1   | 80.2      |
| LLaVA-1.5    | Vicuna-13B   | 26.1      | 36.4 | 1,531.3  | 295.4 | 67.7 | 63.6  | 68.2  | 70.7   | 61.4    | 35.4   | 46.7   | 85.9      |
| ShareGPT4V   | Vicuna-7B    | 25.8      | 36.6 | 1,567.4  | 376.4 | 68.8 | 62.2  | 69.7  | 72.6   | -       | 37.6   | 49.8   | -      |
| CogVLM-17B   | Vicuna-7B    | 34.7      | 37.3 | -        | -     | 65.8 | 55.9  | 68.8  | 73.9   | -       | 54.5   | 55.1   | -      |
| LLaVA-XTuner | InernLM2-20B | 24.6      | 39.4 | -        | -     | 75.1 | 73.7  | 70.2  | 63.7   | -       | 37.2   | 47.7   | -      |
| Monkey-10B   | Qwen-7B      | 34.8      | 40.7 | 1,522.4 | 401.4 | 72.4 | 67.5  | 68.9  | 33.5   | -       | 33.0     | 58.4   | -      |
| InternLM-XC  | InernLM-7B   | 29.5      | 35.6 | 1,528.4  | 391.1 | 74.4 | 72.4  | 66.1  | 53.8   | 64.4    | 35.2   | 57.0   | -      |
| Ours         | InernLM2-7B  | 57.6      | 43.0 | 1,712.0  | 530.7 | 79.6 | 77.6  | 75.9  | 81.8   | 72.5    | 51.2   | 59.1   | 87.7    |

 

## ç¯å¢ƒè¦æ±‚

* python 3.8 and above
* pytorch 1.12 and above, 2.0 and above are recommended
* CUDA 11.4 and above are recommended (this is for GPU users)
  <br>

## å®‰è£…æ•™ç¨‹

åœ¨è¿è¡Œä»£ç ä¹‹å‰ï¼Œè¯·å…ˆæŒ‰ç…§è¦æ±‚é…ç½®ç¯å¢ƒã€‚è¯·ç¡®è®¤ä½ çš„è®¾å¤‡ç¬¦åˆä»¥ä¸Šç¯å¢ƒéœ€æ±‚ï¼Œç„¶åå®‰è£…ç¯å¢ƒã€‚
è¯·å‚è€ƒ[å®‰è£…æ•™ç¨‹](docs/install_CN.md)

## å¿«é€Ÿå¼€å§‹

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•å®ç”¨çš„ ğŸ¤— Transformers ç‰ˆæœ¬ InternLM-XComposer çš„ä½¿ç”¨æ¡ˆä¾‹ã€‚

<details>
  <summary>
    <b>ğŸ¤— Transformers</b>
  </summary>


```python
import torch
from transformers import AutoModel, AutoTokenizer

torch.set_grad_enabled(False)

# init model and tokenizer
model = AutoModel.from_pretrained('internlm/internlm-xcomposer2-vl-7b', trust_remote_code=True).cuda().eval()
tokenizer = AutoTokenizer.from_pretrained('internlm/internlm-xcomposer2-vl-7b', trust_remote_code=True)  
 
text = '<ImageHere>ä»”ç»†æè¿°è¿™å¼ å›¾'
image = 'examples/image1.webp'
with torch.cuda.amp.autocast(): 
  response, _ = model.chat(tokenizer, query=query, image=image, history=[], do_sample=False) 
print(response)
#è¿™å¼ å›¾ç‰‡æ˜¯ä¸€ä¸ªå¼•ç”¨çš„å¥¥æ–¯å¡Â·ç‹å°”å¾·çš„åè¨€ï¼Œå®ƒè¢«æ”¾åœ¨ä¸€ä¸ªç¾ä¸½çš„æ—¥è½èƒŒæ™¯ä¸Šã€‚
#å¼•ç”¨çš„å†…å®¹æ˜¯â€œLive life with no excuses, travel with no regretsâ€ï¼Œæ„æ€æ˜¯â€œç”Ÿæ´»ä¸è¦æ‰¾å€Ÿå£ï¼Œæ—…è¡Œä¸è¦åæ‚”â€ã€‚
# åœ¨æ—¥è½æ—¶åˆ†ï¼Œä¸¤ä¸ªèº«å½±ç«™åœ¨å±±ä¸˜ä¸Šï¼Œä»–ä»¬ä¼¼ä¹æ­£åœ¨äº«å—è¿™ä¸ªç¾æ™¯ã€‚æ•´ä¸ªåœºæ™¯ä¼ è¾¾å‡ºä¸€ç§ç§¯æå‘ä¸Šã€å‹‡æ•¢è¿½æ±‚æ¢¦æƒ³çš„æƒ…æ„Ÿã€‚
```
</details>


<details>
  <summary>
    <b>ğŸ¤– ModelScope</b>
  </summary>


```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer

torch.set_grad_enabled(False)

# init model and tokenizer
model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm-xcomposer2-vl-7b')
model = AutoModel.from_pretrained(model_dir, trust_remote_code=True).cuda().eval()
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model.tokenizer = tokenizer

text = '<ImageHere>ä»”ç»†æè¿°è¿™å¼ å›¾'
image = 'examples/image1.webp'
with torch.cuda.amp.autocast(): 
  response, _ = model.chat(tokenizer, query=query, image=image, history=[], do_sample=False) 
print(response)
#è¿™å¼ å›¾ç‰‡æ˜¯ä¸€ä¸ªå¼•ç”¨çš„å¥¥æ–¯å¡Â·ç‹å°”å¾·çš„åè¨€ï¼Œå®ƒè¢«æ”¾åœ¨ä¸€ä¸ªç¾ä¸½çš„æ—¥è½èƒŒæ™¯ä¸Šã€‚
#å¼•ç”¨çš„å†…å®¹æ˜¯â€œLive life with no excuses, travel with no regretsâ€ï¼Œæ„æ€æ˜¯â€œç”Ÿæ´»ä¸è¦æ‰¾å€Ÿå£ï¼Œæ—…è¡Œä¸è¦åæ‚”â€ã€‚
# åœ¨æ—¥è½æ—¶åˆ†ï¼Œä¸¤ä¸ªèº«å½±ç«™åœ¨å±±ä¸˜ä¸Šï¼Œä»–ä»¬ä¼¼ä¹æ­£åœ¨äº«å—è¿™ä¸ªç¾æ™¯ã€‚æ•´ä¸ªåœºæ™¯ä¼ è¾¾å‡ºä¸€ç§ç§¯æå‘ä¸Šã€å‹‡æ•¢è¿½æ±‚æ¢¦æƒ³çš„æƒ…æ„Ÿã€‚
```
</details>


## Web UI

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªè½»æ¾æ­å»º Web UI demo çš„ä»£ç .
 
```
# è‡ªç”±å½¢å¼çš„å›¾æ–‡åˆ›ä½œdemo
python examples/gradio_demo_composition.py

# å¤šæ¨¡æ€å¯¹è¯demo
python examples/gradio_demo_chat.py
```
æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ Web UI [ç”¨æˆ·æŒ‡å—](demo_asset/demo.md)ã€‚ å¦‚æœæ‚¨æƒ³è¦æ›´æ”¹æ¨¡å‹å­˜æ”¾çš„æ–‡ä»¶å¤¹ï¼Œè¯·ä½¿ç”¨ --folder=new_folder é€‰é¡¹ã€‚
 
<br>

## å¼•ç”¨

å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä¸€ä¸ª star :star: å’Œ å¼•ç”¨ :pencil: :)

```BibTeX
@misc{zhang2023internlmxcomposer,
      title={InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition}, 
      author={Pan Zhang and Xiaoyi Dong and Bin Wang and Yuhang Cao and Chao Xu and Linke Ouyang and Zhiyuan Zhao and Shuangrui Ding and Songyang Zhang and Haodong Duan and Hang Yan and Xinyue Zhang and Wei Li and Jingwen Li and Kai Chen and Conghui He and Xingcheng Zhang and Yu Qiao and Dahua Lin and Jiaqi Wang},
      year={2023},
      eprint={2309.15112},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

<br>

## è®¸å¯è¯ & è”ç³»æˆ‘ä»¬

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œä¹Ÿå¯ç”³è¯·å…è´¹çš„å•†ä¸šä½¿ç”¨æˆæƒï¼ˆ[ç”³è¯·è¡¨](https://wj.qq.com/s2/12725412/f7c1/)ï¼‰ã€‚å…¶ä»–é—®é¢˜ä¸åˆä½œè¯·è”ç³» <internlm@pjlab.org.cn>ã€‚
