<p align="center">
    <img src="./assets/logo_cn.png" width="400"/>
</p>
<p align="center">
    <b><font size="6">æµ¦è¯­Â·çµç¬”2</font></b>
</p>

<!-- <div align="center">
        InternLM-XComposer <a href="">ğŸ¤– <a> <a href="">ğŸ¤—</a>&nbsp ï½œ InternLM-VL <a href="">ğŸ¤– <a> <a href="">ğŸ¤—</a>&nbsp | Technical Report <a href=""> <a> ğŸ“„  -->

<div align="center">
        InternLM-XComposer2 <a href="https://huggingface.co/internlm/internlm-xcomposer2-7b">ğŸ¤—</a> <a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-7b"><img src="./assets/modelscope_logo.png" width="20px"></a> &nbspï½œ InternLM-XComposer2-VL <a href="https://huggingface.co/internlm/internlm-xcomposer2-vl-7b">ğŸ¤—</a> <a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-vl-7b"><img src="./assets/modelscope_logo.png" width="20px"></a> &nbsp | InternLM-XComposer2-<img src="./assets/4k.png" width="25px"> <a href="https://huggingface.co/internlm/internlm-xcomposer2-4khd-7b">ğŸ¤—</a> <a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-4khd-7b"><img src="./assets/modelscope_logo.png" width="20px"></a> &nbsp
</div>
<div align="center">
         çµç¬”2æŠ€æœ¯æŠ¥å‘Š <a href="https://arxiv.org/abs/2401.16420">  ğŸ“„ </a> | çµç¬”2-<img src="./assets/4k.png" width="27px"> æŠ€æœ¯æŠ¥å‘Š <a href="https://arxiv.org/abs/2404.06512">  ğŸ“„ </a>

[English](./README.md) | [ç®€ä½“ä¸­æ–‡](./README_CN.md)

<p align="center">
    æ„Ÿè°¢ç¤¾åŒºæä¾›çš„ InternLM-XComposer2 <a href="https://huggingface.co/spaces/Willow123/InternLM-XComposer">Hugging Face åœ¨çº¿è¯•ç”¨</a> | <a href="https://openxlab.org.cn/apps/detail/WillowBreeze/InternLM-XComposer">OpenXLab åœ¨çº¿è¯•ç”¨</a>
</p>

</div>
<p align="center">
    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href="https://discord.gg/xa29JuW87d" target="_blank">Discord</a> å’Œ <a href="https://r.vansin.top/?r=internwx" target="_blank">å¾®ä¿¡ç¤¾åŒº</a>
</p>

<br>

## æœ¬ä»“åº“åŒ…æ‹¬çš„å¤šæ¨¡æ€é¡¹ç›®

> [**InternLM-XComposer2-<img src="./assets/4k.png" width="25px">**](https://github.com/InternLM/InternLM-XComposer): **A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD**

> [**InternLM-XComposer2**](https://github.com/InternLM/InternLM-XComposer): **Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Models**

> [**InternLM-XComposer**](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-1.0): **A Vision-Language Large Model for Advanced Text-image Comprehension and Composition**

> <img src="https://raw.githubusercontent.com/ShareGPT4V/ShareGPT4V-Resources/master/images/logo_tight.png" style="vertical-align: -20px;" :height="25px" width="25px">[**ShareGPT4V**](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V): **Improving Large Multi-modal Models with Better Captions**
 
> [**DualFocus**](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/DualFocus): **Integrating Macro and Micro Perspectives in Multi-modal Large Language Models**


</br>

**æµ¦è¯­Â·çµç¬”2**æ˜¯åŸºäº[ä¹¦ç”ŸÂ·æµ¦è¯­2](https://github.com/InternLM/InternLM/tree/main)å¤§è¯­è¨€æ¨¡å‹ç ”å‘çš„çªç ´æ€§çš„å›¾æ–‡å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œå…·æœ‰éå‡¡çš„å›¾æ–‡å†™ä½œå’Œå›¾åƒç†è§£èƒ½åŠ›ï¼Œåœ¨å¤šç§åº”ç”¨åœºæ™¯è¡¨ç°å‡ºè‰²ï¼š

- **è‡ªç”±æŒ‡ä»¤è¾“å…¥çš„å›¾æ–‡å†™ä½œï¼š** æµ¦è¯­Â·çµç¬”2å¯ä»¥ç†è§£**è‡ªç”±å½¢å¼çš„å›¾æ–‡æŒ‡ä»¤è¾“å…¥ï¼ŒåŒ…æ‹¬å¤§çº²ã€æ–‡ç« ç»†èŠ‚è¦æ±‚ã€å‚è€ƒå›¾ç‰‡ç­‰**ï¼Œä¸ºç”¨æˆ·æ‰“é€ å›¾æ–‡å¹¶è²Œçš„ä¸“å±æ–‡ç« ã€‚ç”Ÿæˆçš„æ–‡ç« æ–‡é‡‡æ–ç„¶ï¼Œå›¾æ–‡ç›¸å¾—ç›Šå½°ï¼Œæä¾›æ²‰æµ¸å¼çš„é˜…è¯»ä½“éªŒã€‚

- **å‡†ç¡®çš„å›¾æ–‡é—®é¢˜è§£ç­”ï¼š** æµ¦è¯­Â·çµç¬”2å…·æœ‰æµ·é‡å›¾æ–‡çŸ¥è¯†ï¼Œå¯ä»¥å‡†ç¡®çš„å›å¤å„ç§å›¾æ–‡é—®ç­”éš¾é¢˜ï¼Œåœ¨è¯†åˆ«ã€æ„ŸçŸ¥ã€ç»†èŠ‚æè¿°ã€è§†è§‰æ¨ç†ç­‰èƒ½åŠ›ä¸Šè¡¨ç°æƒŠäººã€‚

- **æ°å‡ºæ€§èƒ½ï¼š** æµ¦è¯­Â·çµç¬”2åŸºäºä¹¦ç”ŸÂ·æµ¦è¯­2-7Bæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨13é¡¹å¤šæ¨¡æ€è¯„æµ‹ä¸­å¤§å¹…é¢†å…ˆåŒé‡çº§å¤šæ¨¡æ€æ¨¡å‹ï¼Œåœ¨å…¶ä¸­6é¡¹è¯„æµ‹ä¸­è¶…è¿‡ GPT-4V å’Œ Gemini Proã€‚

<p align="center">
    <img src="assets/Benchmark_radar_CN.png" width="1000"/>
</p>


**InternLM-XComposer2-4KHD** è¿›ä¸€æ­¥æ”¯æŒ4Kåˆ†è¾¨ç‡è¾“å…¥å’Œæ–‡æ¡£ç†è§£ã€‚
<p align="center">
    <img src="assets/4khd_radar.png" width="500"/>
</p>




æˆ‘ä»¬å¼€æºçš„ æµ¦è¯­Â·çµç¬”2 åŒ…æ‹¬ä¸¤ä¸ªç‰ˆæœ¬:

- **InternLM-XComposer2-4KHD-7B** <a href="https://huggingface.co/internlm/internlm-xcomposer2-4khd-7b">ğŸ¤—</a>: åŸºäºæµ¦è¯­Â·çµç¬”2-7Bå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œæ”¯æŒ4Kåˆ†è¾¨ç‡è¾“å…¥ï¼Œé¢å‘å¤šæ¨¡æ€è¯„æµ‹å’Œè§†è§‰é—®ç­”

- **InternLM-XComposer2-VL-7B** <a href="https://huggingface.co/internlm/internlm-xcomposer2-vl-7b">ğŸ¤—</a> <a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-vl-7b"><img src="./assets/modelscope_logo.png" width="20px"> </a>ï¼ˆæµ¦è¯­Â·çµç¬”2-è§†è§‰é—®ç­”-7Bï¼‰: åŸºäºä¹¦ç”ŸÂ·æµ¦è¯­2-7Bå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒï¼Œé¢å‘å¤šæ¨¡æ€è¯„æµ‹å’Œè§†è§‰é—®ç­”ã€‚æµ¦è¯­Â·çµç¬”2-è§†è§‰é—®ç­”-7Bæ˜¯ç›®å‰æœ€å¼ºçš„åŸºäº7Bé‡çº§è¯­è¨€æ¨¡å‹åŸºåº§çš„å›¾æ–‡å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œé¢†è·‘å¤šè¾¾13ä¸ªå¤šæ¨¡æ€å¤§æ¨¡å‹æ¦œå•ã€‚

- **InternLM-XComposer2-VL-1.8B** <a href="https://huggingface.co/internlm/internlm-xcomposer2-vl-1_8b">ğŸ¤—</a> : åŸºäºInternLM-1.8Bçš„è½»é‡åŒ–å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œå…¼é¡¾äº†æ€§èƒ½å’Œæ•ˆç‡.

- **InternLM-XComposer2-7B** <a href="https://huggingface.co/internlm/internlm-xcomposer2-7b">ğŸ¤—</a> <a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-7b/"><img src="./assets/modelscope_logo.png" width="20px"> </a>: è¿›ä¸€æ­¥å¾®è°ƒï¼Œæ”¯æŒè‡ªç”±æŒ‡ä»¤è¾“å…¥å›¾æ–‡å†™ä½œçš„å›¾æ–‡å¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚

æ›´å¤šæ–¹æ³•ç»†èŠ‚è¯·å‚è€ƒ[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2401.16420) å’Œ[4KHDæŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/pdf/2404.06512.pdf)ï¼
<br>

<!--
<p align="center">
    <figcaption align = "center"><b> InternLM-XComposer </b></figcaption>
<p> -->

<!-- ## Demo


https://github.com/InternLM/InternLM-XComposer/assets/22662425/0a2b475b-3f74-4f41-a5df-796680fa56cd
 -->

## Demo Video

[https://github.com/InternLM/InternLM-XComposer/assets/22662425/0a2b475b-3f74-4f41-a5df-796680fa56cd](https://github.com/InternLM/InternLM-XComposer/assets/30363822/63756590-7366-4c5d-807f-66c4e69ea827)

## æ›´æ–°æ¶ˆæ¯
- `2024.04.09` ğŸ‰ğŸ‰ğŸ‰ æˆ‘ä»¬å¼€æºäº†[InternLM-XComposer2-4KHD-7B](https://huggingface.co/internlm/internlm-xcomposer2-4khd-7b) å’Œ [è¯„æµ‹ä»£ç ](./evaluation/README.md).
- `2024.04.09` ğŸ‰ğŸ‰ğŸ‰ æˆ‘ä»¬å¼€æºäº†[InternLM-XComposer2-VL-1.8B](https://huggingface.co/internlm/internlm-xcomposer2-4khd-7b).
- `2024.02.22` ğŸ‰ğŸ‰ğŸ‰ æˆ‘ä»¬å¼€æºäº†[DualFocus](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/DualFocus), ä¸€ä¸ªæ•´åˆå®è§‚å’Œå¾®è§‚è§†è§’äºå¤šè¯­è¨€å¤§æ¨¡å‹ä¸­ä»¥æå‡è§†è§‰-è¯­è¨€ä»»åŠ¡æ€§èƒ½çš„æ¡†æ¶ã€‚
* ```2024.02.06``` ğŸ‰ğŸ‰ğŸ‰ [InternLM-XComposer2-7B-4bit](https://huggingface.co/internlm/internlm-xcomposer2-7b-4bit) å’Œ [InternLM-XComposer-VL2-7B-4bit](https://huggingface.co/internlm/internlm-xcomposer2-vl-7b-4bit) å·²åœ¨**Hugging Face**å’Œ**ModelScope**å¼€æºã€‚
- `2024.02.02` ğŸ‰ğŸ‰ğŸ‰ **InternLM-XComposer2-VL-7B**çš„[å¾®è°ƒä»£ç ](./finetune/)å·²å¼€æºã€‚
- `2024.01.26` ğŸ‰ğŸ‰ğŸ‰ **InternLM-XComposer2-VL-7B**çš„[è¯„æµ‹ä»£ç ](./evaluation/README.md)å·²å¼€æºã€‚
- `2024.01.26` ğŸ‰ğŸ‰ğŸ‰ [InternLM-XComposer2-7B](https://huggingface.co/internlm/internlm-xcomposer2-7b) å’Œ [InternLM-XComposer-VL2-7B](https://huggingface.co/internlm/internlm-xcomposer2-vl-7b)å·²åœ¨**Hugging Face**å’Œ**ModelScope**å¼€æºã€‚
- `2024.01.26` ğŸ‰ğŸ‰ğŸ‰ æˆ‘ä»¬å…¬å¼€äº†InternLM-XComposer2æ›´å¤šæŠ€æœ¯ç»†èŠ‚ï¼Œè¯·å‚è€ƒ[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2401.16420)ã€‚
- `2023.11.22` ğŸ‰ğŸ‰ğŸ‰ æˆ‘ä»¬å¼€æºäº†[ShareGPT4V](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V), ä¸€ä¸ªé«˜è´¨é‡çš„å¤§è§„æ¨¡å›¾æ–‡æè¿°æ•°æ®é›†ï¼Œä»¥åŠæ€§èƒ½ä¼˜ç§€çš„å¤šæ¨¡æ€å¤§æ¨¡å‹ShareGPT4V-7Bã€‚
- `2023.10.30` ğŸ‰ğŸ‰ğŸ‰ çµç¬”åœ¨[Q-Bench](https://github.com/Q-Future/Q-Bench/tree/master/leaderboards#overall-leaderboards) å’Œ [Tiny LVLM](https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/tiny_lvlm_evaluation) å–å¾—äº†ç¬¬ä¸€åã€‚
- `2023.10.19` ğŸ‰ğŸ‰ğŸ‰ æ”¯æŒå¤šå¡æµ‹è¯•ï¼Œå¤šå¡Demo. ä¸¤å¼ 4090æ˜¾å¡å¯éƒ¨ç½²å…¨é‡Demoã€‚
- `2023.10.12` ğŸ‰ğŸ‰ğŸ‰ æ”¯æŒ4æ¯”ç‰¹é‡åŒ–Demoï¼Œ æ¨¡å‹æ–‡ä»¶å¯ä»[Hugging Face](https://huggingface.co/internlm/internlm-xcomposer-7b-4bit) å’Œ [ModelScope](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-7b-4bit) è·å–ã€‚
- `2023.10.8` ğŸ‰ğŸ‰ğŸ‰ [InternLM-XComposer-7B](https://huggingface.co/internlm/internlm-xcomposer-7b) å’Œ [InternLM-XComposer-VL-7B](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-vl-7b) å·²åœ¨Modelscopeå¼€æºã€‚
- `2023.9.27` ğŸ‰ğŸ‰ğŸ‰ **InternLM-XComposer-VL-7B**çš„[è¯„æµ‹ä»£ç ](./InternLM-XComposer-1.0/evaluation/)å·²å¼€æºã€‚
- `2023.9.27` ğŸ‰ğŸ‰ğŸ‰ [InternLM-XComposer-7B](https://huggingface.co/internlm/internlm-xcomposer-7b) å’Œ [InternLM-XComposer-VL-7B](https://huggingface.co/internlm/internlm-xcomposer-vl-7b) å·²åœ¨Hugging Faceå¼€æºã€‚
- `2023.9.27` ğŸ‰ğŸ‰ğŸ‰ æ›´å¤šæŠ€æœ¯ç»†èŠ‚è¯·å‚è€ƒ[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/pdf/2309.15112.pdf)ã€‚
  </br>

## æ¨¡å‹åˆé›†

| æ¨¡å‹                        | ç”¨é€”                | Transformers(HF)                                                                           | ModelScope(HF)                                                                                                                                                               | å¼€æºæ—¥æœŸ   |
| --------------------------- | ------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- |
| **InternLM-XComposer2-4KHD**     | 4Kåˆ†è¾¨ç‡å›¾åƒç†è§£, Benchmark, è§†è§‰é—®ç­”          | [ğŸ¤—internlm-xcomposer2-4khd-7b](https://huggingface.co/internlm/internlm-xcomposer2-4khd-7b)         | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm-xcomposer2-4khd-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-4khd-7b/summary)         | 2024-04-09   |
| **InternLM-XComposer2-VL-1.8B**  | Benchmark, è§†è§‰é—®ç­”             | [ğŸ¤—internlm-xcomposer2-vl-1_8b](https://huggingface.co/internlm/internlm-xcomposer2-vl-1_8b)   | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm-xcomposer2-vl-1_8b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-vl-1_8b/summary)   | 2024-04-09   |
| **InternLM-XComposer2**     | å›¾æ–‡åˆ›ä½œ            | [ğŸ¤—internlm-xcomposer2-7b](https://huggingface.co/internlm/internlm-xcomposer2-7b)         | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm-xcomposer2-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-7b/summary)         | 2024-01-26 |
| **InternLM-XComposer2-VL**  | Benchmark, è§†è§‰é—®ç­” | [ğŸ¤—internlm-xcomposer2-vl-7b](https://huggingface.co/internlm/internlm-xcomposer2-vl-7b)   | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm-xcomposer2-vl-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-vl-7b/summary)   | 2024-01-26 |
| **InternLM-XComposer2-4bit**  |  å›¾æ–‡åˆ›ä½œ   | [ğŸ¤—internlm-xcomposer2-7b-4bit](https://huggingface.co/internlm/internlm-xcomposer2-7b-4bit) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm-xcomposer2-7b-4bit](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-7b-4bit/summary) |  2024-02-06   |
| **InternLM-XComposer2-VL-4bit**   | Benchmark, è§†è§‰é—®ç­”   | [ğŸ¤—internlm-xcomposer2-vl-7b-4bit](https://huggingface.co/internlm/internlm-xcomposer2-vl-7b-4bit) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm-xcomposer2-vl-7b-4bit](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer2-vl-7b-4bit/summary) |  2024-02-06   |
| **InternLM-XComposer**      | å›¾æ–‡åˆ›ä½œ, è§†è§‰é—®ç­”  | [ğŸ¤—internlm-xcomposer-7b](https://huggingface.co/internlm/internlm-xcomposer-7b)           | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm-xcomposer-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-7b/summary)           | 2023-09-26 |
| **InternLM-XComposer-4bit** | å›¾æ–‡åˆ›ä½œ, è§†è§‰é—®ç­”  | [ğŸ¤—internlm-xcomposer-7b-4bit](https://huggingface.co/internlm/internlm-xcomposer-7b-4bit) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm-xcomposer-7b-4bit](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-7b-4bit/summary) | 2023-09-26 |
| **InternLM-XComposer-VL**   | Benchmark           | [ğŸ¤—internlm-xcomposer-vl-7b](https://huggingface.co/internlm/internlm-xcomposer-vl-7b)     | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm-xcomposer-vl-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-xcomposer-vl-7b/summary)     | 2023-09-26 |

## è¯„æµ‹

æˆ‘ä»¬åœ¨16ä¸ªå¤šæ¨¡æ€è¯„æµ‹å¯¹InternLM-XComposer2-VLä¸Šè¿›è¡Œæµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š[MMStar](https://github.com/MMStar-Benchmark/MMStar), [DocVQA](https://rrc.cvc.uab.es/?ch=17), [Infographics VQA](https://rrc.cvc.uab.es/?ch=17), [TextVQA](https://textvqa.org/), [ChartQA](https://github.com/vis-nlp/ChartQA), [OCRBench](https://github.com/Yuliang-Liu/MultimodalOCR), [MathVista](https://mathvista.github.io/), [MMMU](https://mmmu-benchmark.github.io/), [AI2D](https://prior.allenai.org/projects/diagram-understanding), [MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation), [MMBench](https://opencompass.org.cn/leaderboard-multimodal), [MMBench-CN](https://opencompass.org.cn/leaderboard-multimodal), [SEED-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard), [QBench](https://github.com/Q-Future/Q-Bench/tree/master/leaderboards#overall-leaderboards), [HallusionBench](https://github.com/tianyi-lab/HallusionBench), [MM-Vet](https://github.com/yuweihao/MM-Vet).

å¤ç°è¯„æµ‹ç»“æœï¼Œè¯·å‚è€ƒ[è¯„æµ‹ç»†èŠ‚](./evaluation/README.md)ã€‚

### å¯¹æ¯”é—­æºå¤šæ¨¡æ€APIä»¥åŠå¼€æºSOTAæ¨¡å‹ã€‚

|                          | DocVQA     | ChartVQA   | InfoVQA    | TextVQA  | OCRBench | MMStar  | MathVista | AI2D    | MMMU   | MME     | MMB     | MMBCN   | SEEDI   | QBenchT | MM-Vet | HallB   |
|--------------------------|------------|------------|------------|----------|----------|---------|-----------|---------|--------|---------|---------|---------|---------|---------|--------|---------|
|  Open-source Previous SOTA   | DocOwl 1.5 | DocOwl 1.5 | DocOwl 1.5 | CogAgent | CogAgent | LLaVA-N | LLaVA-N   | LLaVA-N | Int-VL | WeMM    | LLaVA-N | LLaVA-N | LLaVA-N | Int-XC  | CogVLM | Monkey  |
|                          | 8B         | 8B         | 8B         | 18B      | 18B      | 35B     | 35B       | 35B     | 40B    | 6B      | 35B     | 35B     | 35B     | 8B      | 17B    | 10B     |
|                          | 82.2       | 70.2       | 44.5       | 76.1     | 59.0     | 52.1    | 39.0      | 78.9    | 51.6   | 2,050.2 | 81.1    | 79.0    | 75.7    | 64.4    | 54.5   | 39.3    |
|                          |            |            |            |          |          |         |           |         |        |         |         |         |         |         |        |         |
| GPT-4V                   | 88.4       | 78.5       | 75.1       | 78.0     | 51.6     | 57.1    | 47.8      | 75.5    | 56.8   | 1,926.5 | 77.0    | 74.4    | 69.1    | 74.1    | 56.8   | 46.5    |
| Gemini-Pro               | 88.1       | 74.1       | 75.2       | 74.6     | 68.0     | 42.6    | 45.8      | 70.2    | 47.9   | 1,933.3 | 73.6    | 74.3    | 70.7    | 70.6    | 59.2   | 45.2    |
| InternLM-XComposer2-VL   | 57.7       | 72.6       | 34.4       | 70.1     | 53.2     | 55.4    | 57.6      | 81.2    | 41.4   | 2,220.4 | 80.7    | 79.4    | 74.9    | 72.5    | 46.7   | 41.0    |
| InternLM-XComposer2-4KHD | 90.0       | 81.0       | 68.6       | 77.2     | 67.5     | 54.1    | 57.8      | 80.9    | 39.9   | 2,204.9 | 80.2    | 77.7    | 74.7    | 71.8    | 54.9   | 40.9    |

### å¯¹æ¯”å¼€æºæ¨¡å‹ã€‚

| Method       | LLM          | MMStar | MathVista | AI2D | MMEP    | MMEC  | MMB  | MMBCN | SEEDI | QBenchT | MM-Vet  |
|--------------|--------------|--------|-----------|------|---------|-------|------|-------|-------|---------|---------|
| InstructBLIP | Vicuna-7B    | ---    | 25.3      | 40.6 | -       | -     | 36.0 | 23.7  | 53.4  | 55.9    | 26.2    |
| Qwen-VL-Chat | Qwen-7B      | 37.5   | 33.8      | 63.0 | 1,487.5 | 360.7 | 60.6 | 56.7  | 58.2  | 61.7    | 47.3    |
| LLaVA-1.5    | Vicuna-13B   | 13.9   | 26.1      | 61.1 | 1,531.3 | 295.4 | 67.7 | 63.6  | 68.2  | 61.4    | 35.4    |
| ShareGPT4V   | Vicuna-7B    | 11.9   | 25.8      | 58.0 | 1,567.4 | 376.4 | 68.8 | 62.2  | 69.7  | -       | 37.6    |
| CogVLM-17B   | Vicuna-7B    | 14.9   | 34.7      | 63.3 | -       | -     | 65.8 | 55.9  | 68.8  | -       | 54.5    |
| LLaVA-XTuner | InernLM2-20B | ---    | 24.6      | 65.4 | -       | -     | 75.1 | 73.7  | 70.2  | -       | 37.2    |
| Monkey       | Qwen-7B      | 38.3   | 34.8      | 62.5 | 1,522.4 | 401.4 | 72.4 | 67.5  | 68.9  | -       | 33      |
| LLaVA-Next   | Vicuna-13B   | 38.3   | 32.4      | 72.2 | 1,445.0 | 296.0 | 70.0 | 68.5  | 71.4  | -       | 44.9    |
| InternLM-XC  | InernLM-7B   | ---    | 29.5      | 56.9 | 1,528.4 | 391.1 | 74.4 | 72.4  | 66.1  | 64.4    | 35.2    |
| InternLM-XComposer2-VL      | InernLM2-7B  | 55.4   | 57.6      | 81.2 | 1,712.0 | 530.7 | 80.7 | 79.4  | 74.9  | 72.5    | 46.7    |
| InternLM-XComposer2-4KHD    | InernLM2-7B  | 54.1   | 57.8      | 80.9 | 1,655.9 | 548.9 | 80.2 | 77.7  | 74.7  | 71.8    | 54.9    |


| Method     | LLM               | MMStar | MathVista | MMMU | MMEP    | MMEC  | CCBench | MMB  | SEEDI | MM-Vet | HallB | ChartQA | OCRBench | TextVQA | DocVQA | InfoVQA  |
|------------|-------------------|--------|-----------|------|---------|-------|---------|------|-------|--------|-------|---------|----------|---------|--------|----------|
| MobileVLM  |  MobileLLaMA 2.7B  |---  | ---       | ---  | 1,288.9 | ---   | ---     | 59.6 | ---   | ---    | ---   | ---     | ---      | ---     | ---    | ---      |
| LLaVA-Phi  |  Phi2-2.7B  |---  | ---       | ---  | 1,335.1 | ---   | ---     | 59.8 | ---   | ---    | ---   | ---     | ---      | ---     | ---    | ---      |
| MoE-LLaVA  | 4x Phi-2 2.7B |---  | ---       | ---  | 1,431.3 | ---   | ---     | 68.0 | ---   | ---    | ---   | ---     | ---      | ---     | ---    | ---      |
| TinyLLaVA  | Phi2-2.7B         | 36.0   | ---       | ---  | 1,464.9 | ---   | ---     | 66.9 | ---   | 32.0   | ---   | ---     | ---      | ---     | ---    | ---      |
|            |                   |        |           |      |         |       |         |      |       |        |       |         |          |         |        |          |
| InternLM-XComposer2-VL       | InernLM2-1.8B     | 46.3   | 48.2      | 30.1 | 1,465.9 | 420.0 | 41.4    | 72.5 | 70.4  | 30.1   | 34.4  | 57.8    | 46.0     | 65.9    | 48.3   | 24.1     |


## ç¯å¢ƒè¦æ±‚

- python 3.8 and above
- pytorch 1.12 and above, 2.0 and above are recommended
- CUDA 11.4 and above are recommended (this is for GPU users)
- [flash-attention2](https://github.com/Dao-AILab/flash-attention) is required for the 4KHD model.
  <br>

## å®‰è£…æ•™ç¨‹

åœ¨è¿è¡Œä»£ç ä¹‹å‰ï¼Œè¯·å…ˆæŒ‰ç…§è¦æ±‚é…ç½®ç¯å¢ƒã€‚è¯·ç¡®è®¤ä½ çš„è®¾å¤‡ç¬¦åˆä»¥ä¸Šç¯å¢ƒéœ€æ±‚ï¼Œç„¶åå®‰è£…ç¯å¢ƒã€‚
è¯·å‚è€ƒ[å®‰è£…æ•™ç¨‹](docs/install_CN.md)

## å¿«é€Ÿå¼€å§‹

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•å®ç”¨çš„ ğŸ¤— Transformers ç‰ˆæœ¬ InternLM-XComposerç³»åˆ—çš„ä½¿ç”¨æ¡ˆä¾‹ã€‚ 

### XComposer2-4KHD
<details>
  <summary>
    <b>ğŸ¤— Transformers</b>
  </summary>

```python
import torch
from transformers import AutoModel, AutoTokenizer

torch.set_grad_enabled(False)

# init model and tokenizer
model = AutoModel.from_pretrained('internlm/internlm-xcomposer2-4khd-7b', torch_dtype=torch.bfloat16, trust_remote_code=True).cuda().eval()
tokenizer = AutoTokenizer.from_pretrained('internlm/internlm-xcomposer2-4khd-7b', trust_remote_code=True)

###############
# First Round
###############
query = '<ImageHere>Illustrate the fine details present in the image'
image = 'examples/4khd_example.webp'
with torch.cuda.amp.autocast():
  response, his = model.chat(tokenizer, query=query, image=image, hd_num=55, history=[], do_sample=False, num_beams=3)
print(response)
# The image is a vibrant and colorful infographic that showcases 7 graphic design trends that will dominate in 2021. The infographic is divided into 7 sections, each representing a different trend. 
# Starting from the top, the first section focuses on "Muted Color Palettes", highlighting the use of muted colors in design.
# The second section delves into "Simple Data Visualizations", emphasizing the importance of easy-to-understand data visualizations. 
# The third section introduces "Geometric Shapes Everywhere", showcasing the use of geometric shapes in design. 
# The fourth section discusses "Flat Icons and Illustrations", explaining how flat icons and illustrations are being used in design. 
# The fifth section is dedicated to "Classic Serif Fonts", illustrating the resurgence of classic serif fonts in design.
# The sixth section explores "Social Media Slide Decks", illustrating how slide decks are being used on social media. 
# Finally, the seventh section focuses on "Text Heavy Videos", illustrating the trend of using text-heavy videos in design. 
# Each section is filled with relevant images and text, providing a comprehensive overview of the 7 graphic design trends that will dominate in 2021.

###############
# Second Round
###############
query1 = 'what is the detailed explanation of the third part.'
with torch.cuda.amp.autocast():
  response, _ = model.chat(tokenizer, query=query1, image=image, hd_num=55, history=his, do_sample=False, num_beams=3)
print(response)
# The third part of the infographic is about "Geometric Shapes Everywhere". It explains that last year, designers used a lot of
# flowing and abstract shapes in their designs. However, this year, they have been replaced with rigid, hard-edged geometric
# shapes and patterns. The hard edges of a geometric shape create a great contrast against muted colors.


```

</details>

<details>
  <summary>
    <b>ğŸ¤– ModelScope</b>
  </summary>

```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer

torch.set_grad_enabled(False)

# init model and tokenizer
model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm-xcomposer2-4khd-7b')
model = AutoModel.from_pretrained(model_dir, trust_remote_code=True).cuda().eval()
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)

###############
# First Round
###############
query = '<ImageHere>Illustrate the fine details present in the image'
image = 'examples/4khd_example.webp'
with torch.cuda.amp.autocast():
  response, his = model.chat(tokenizer, query=query, image=image, hd_num=55, history=[], do_sample=False, num_beams=3)
print(response)
# The image is a vibrant and colorful infographic that showcases 7 graphic design trends that will dominate in 2021. The infographic is divided into 7 sections, each representing a different trend. 
# Starting from the top, the first section focuses on "Muted Color Palettes", highlighting the use of muted colors in design.
# The second section delves into "Simple Data Visualizations", emphasizing the importance of easy-to-understand data visualizations. 
# The third section introduces "Geometric Shapes Everywhere", showcasing the use of geometric shapes in design. 
# The fourth section discusses "Flat Icons and Illustrations", explaining how flat icons and illustrations are being used in design. 
# The fifth section is dedicated to "Classic Serif Fonts", illustrating the resurgence of classic serif fonts in design.
# The sixth section explores "Social Media Slide Decks", illustrating how slide decks are being used on social media. 
# Finally, the seventh section focuses on "Text Heavy Videos", illustrating the trend of using text-heavy videos in design. 
# Each section is filled with relevant images and text, providing a comprehensive overview of the 7 graphic design trends that will dominate in 2021.

###############
# Second Round
###############
query1 = 'what is the detailed explanation of the third part.'
with torch.cuda.amp.autocast():
  response, _ = model.chat(tokenizer, query=query1, image=image, hd_num=55, history=his, do_sample=False, num_beams=3)
print(response)
# The third part of the infographic is about "Geometric Shapes Everywhere". It explains that last year, designers used a lot of
# flowing and abstract shapes in their designs. However, this year, they have been replaced with rigid, hard-edged geometric
# shapes and patterns. The hard edges of a geometric shape create a great contrast against muted colors.


```

</details>

### XComposer2-VL



<details>
  <summary>
    <b>ğŸ¤— Transformers</b>
  </summary>

```python
import torch
from transformers import AutoModel, AutoTokenizer

torch.set_grad_enabled(False)

# init model and tokenizer
model = AutoModel.from_pretrained('internlm/internlm-xcomposer2-vl-7b', trust_remote_code=True).cuda().eval()
tokenizer = AutoTokenizer.from_pretrained('internlm/internlm-xcomposer2-vl-7b', trust_remote_code=True)

text = '<ImageHere>ä»”ç»†æè¿°è¿™å¼ å›¾'
image = 'examples/image1.webp'
with torch.cuda.amp.autocast():
  response, _ = model.chat(tokenizer, query=text, image=image, history=[], do_sample=False)
print(response)
#è¿™å¼ å›¾ç‰‡æ˜¯ä¸€ä¸ªå¼•ç”¨çš„å¥¥æ–¯å¡Â·ç‹å°”å¾·çš„åè¨€ï¼Œå®ƒè¢«æ”¾åœ¨ä¸€ä¸ªç¾ä¸½çš„æ—¥è½èƒŒæ™¯ä¸Šã€‚
#å¼•ç”¨çš„å†…å®¹æ˜¯â€œLive life with no excuses, travel with no regretsâ€ï¼Œæ„æ€æ˜¯â€œç”Ÿæ´»ä¸è¦æ‰¾å€Ÿå£ï¼Œæ—…è¡Œä¸è¦åæ‚”â€ã€‚
# åœ¨æ—¥è½æ—¶åˆ†ï¼Œä¸¤ä¸ªèº«å½±ç«™åœ¨å±±ä¸˜ä¸Šï¼Œä»–ä»¬ä¼¼ä¹æ­£åœ¨äº«å—è¿™ä¸ªç¾æ™¯ã€‚æ•´ä¸ªåœºæ™¯ä¼ è¾¾å‡ºä¸€ç§ç§¯æå‘ä¸Šã€å‹‡æ•¢è¿½æ±‚æ¢¦æƒ³çš„æƒ…æ„Ÿã€‚
```

</details>

<details>
  <summary>
    <b>ğŸ¤– ModelScope</b>
  </summary>

```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer

torch.set_grad_enabled(False)

# init model and tokenizer
model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm-xcomposer2-vl-7b')
model = AutoModel.from_pretrained(model_dir, trust_remote_code=True).cuda().eval()
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model.tokenizer = tokenizer

text = '<ImageHere>ä»”ç»†æè¿°è¿™å¼ å›¾'
image = 'examples/image1.webp'
with torch.cuda.amp.autocast():
  response, _ = model.chat(tokenizer, query=text, image=image, history=[], do_sample=False)
print(response)
#è¿™å¼ å›¾ç‰‡æ˜¯ä¸€ä¸ªå¼•ç”¨çš„å¥¥æ–¯å¡Â·ç‹å°”å¾·çš„åè¨€ï¼Œå®ƒè¢«æ”¾åœ¨ä¸€ä¸ªç¾ä¸½çš„æ—¥è½èƒŒæ™¯ä¸Šã€‚
#å¼•ç”¨çš„å†…å®¹æ˜¯â€œLive life with no excuses, travel with no regretsâ€ï¼Œæ„æ€æ˜¯â€œç”Ÿæ´»ä¸è¦æ‰¾å€Ÿå£ï¼Œæ—…è¡Œä¸è¦åæ‚”â€ã€‚
# åœ¨æ—¥è½æ—¶åˆ†ï¼Œä¸¤ä¸ªèº«å½±ç«™åœ¨å±±ä¸˜ä¸Šï¼Œä»–ä»¬ä¼¼ä¹æ­£åœ¨äº«å—è¿™ä¸ªç¾æ™¯ã€‚æ•´ä¸ªåœºæ™¯ä¼ è¾¾å‡ºä¸€ç§ç§¯æå‘ä¸Šã€å‹‡æ•¢è¿½æ±‚æ¢¦æƒ³çš„æƒ…æ„Ÿã€‚
```

</details>

## å¤šGPUæµ‹è¯•
å¦‚æœä½ æœ‰å¤šå¼  GPUï¼Œä½†æ˜¯æ¯å¼  GPU çš„æ˜¾å­˜å¤§å°éƒ½ä¸è¶³ä»¥å®¹çº³å®Œæ•´çš„æ¨¡å‹ï¼Œé‚£ä¹ˆå¯ä»¥å°†æ¨¡å‹åˆ‡åˆ†åœ¨å¤šå¼ GPUä¸Šã€‚é¦–å…ˆå®‰è£… accelerate: pip install accelerateï¼Œç„¶åæ‰§è¡Œä»¥ä¸‹è„šæœ¬è¿›è¡ŒèŠå¤©ï¼š
```
# chat with 2 GPUs
python examples/example_chat.py --num_gpus 2
```


## 4-Bit é‡åŒ–æ¨¡å‹

æˆ‘ä»¬æä¾›4bité‡åŒ–æ¨¡å‹æ¥ç¼“è§£æ¨¡å‹çš„å†…å­˜éœ€æ±‚ã€‚ è¦è¿è¡Œ4bitæ¨¡å‹ï¼ˆGPUå†…å­˜> = 12GBï¼‰ï¼Œæ‚¨éœ€è¦é¦–å…ˆå®‰è£…ç›¸åº”çš„[ä¾èµ–åŒ…](https://github.com/InternLM/InternLM-XComposer/docs/install.md), ç„¶åæ‰§è¡Œä»¥ä¸‹è„šæœ¬:


<details>
  <summary>
    <b>ğŸ¤— Transformers</b>
  </summary>

```python
import torch, auto_gptq
from transformers import AutoModel, AutoTokenizer 
from auto_gptq.modeling import BaseGPTQForCausalLM

auto_gptq.modeling._base.SUPPORTED_MODELS = ["internlm"]
torch.set_grad_enabled(False)

class InternLMXComposer2QForCausalLM(BaseGPTQForCausalLM):
    layers_block_name = "model.layers"
    outside_layer_modules = [
        'vit', 'vision_proj', 'model.tok_embeddings', 'model.norm', 'output', 
    ]
    inside_layer_modules = [
        ["attention.wqkv.linear"],
        ["attention.wo.linear"],
        ["feed_forward.w1.linear", "feed_forward.w3.linear"],
        ["feed_forward.w2.linear"],
    ]
 
# init model and tokenizer
model = InternLMXComposer2QForCausalLM.from_quantized(
  'internlm/internlm-xcomposer2-vl-7b-4bit', trust_remote_code=True, device="cuda:0").eval()
tokenizer = AutoTokenizer.from_pretrained(
  'internlm/internlm-xcomposer2-vl-7b-4bit', trust_remote_code=True)

text = '<ImageHere>Please describe this image in detail.'
image = 'examples/image1.webp'
with torch.cuda.amp.autocast(): 
  response, _ = model.chat(tokenizer, query=query, image=image, history=[], do_sample=False) 
print(response)
#The image features a quote by Oscar Wilde, "Live life with no excuses, travel with no regrets." 
#The quote is displayed in white text against a dark background. In the foreground, there are two silhouettes of people standing on a hill at sunset. 
#They appear to be hiking or climbing, as one of them is holding a walking stick. 
#The sky behind them is painted with hues of orange and purple, creating a beautiful contrast with the dark figures.
```
</details>

## å¾®è°ƒä»£ç 

è¯·å‚è€ƒ [å¾®è°ƒæŒ‡å—](finetune/README_zh-CN.md)

## Web UI

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªè½»æ¾æ­å»º Web UI demo çš„ä»£ç .

```
# è‡ªç”±å½¢å¼çš„å›¾æ–‡åˆ›ä½œdemo
python examples/gradio_demo_composition.py

# å¤šæ¨¡æ€å¯¹è¯demo
python examples/gradio_demo_chat.py
```

æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ Web UI [ç”¨æˆ·æŒ‡å—](demo_asset/demo.md)ã€‚ å¦‚æœæ‚¨æƒ³è¦æ›´æ”¹æ¨¡å‹å­˜æ”¾çš„æ–‡ä»¶å¤¹ï¼Œè¯·ä½¿ç”¨ --folder=new_folder é€‰é¡¹ã€‚

<br>

## å¼•ç”¨

å¦‚æœä½ è§‰å¾—æˆ‘ä»¬æ¨¡å‹/ä»£ç /æŠ€æœ¯æŠ¥å‘Šå¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ â­ å’Œ å¼•ç”¨ ğŸ“ï¼Œè°¢è°¢ :)
```BibTeX
@article{internlmxcomposer2_4khd,
      title={InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD},
      author={Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Bin Wang and Linke Ouyang and Songyang Zhang and Haodong Duan and Wenwei Zhang and Yining Li and Hang Yan and Yang Gao and Zhe Chen and Xinyue Zhang and Wei Li and Jingwen Li and Wenhai Wang and Kai Chen and Conghui He and Xingcheng Zhang and Jifeng Dai and Yu Qiao and Dahua Lin and Jiaqi Wang},
      journal={arXiv preprint arXiv:2404.06512},
      year={2024}
}
```

```BibTeX
@article{internlmxcomposer2,
      title={InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model},
      author={Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Bin Wang and Linke Ouyang and Xilin Wei and Songyang Zhang and Haodong Duan and Maosong Cao and Wenwei Zhang and Yining Li and Hang Yan and Yang Gao and Xinyue Zhang and Wei Li and Jingwen Li and Kai Chen and Conghui He and Xingcheng Zhang and Yu Qiao and Dahua Lin and Jiaqi Wang},
      journal={arXiv preprint arXiv:2401.16420},
      year={2024}
}
```

```BibTeX
@article{internlmxcomposer,
      title={InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition},
      author={Pan Zhang and Xiaoyi Dong and Bin Wang and Yuhang Cao and Chao Xu and Linke Ouyang and Zhiyuan Zhao and Shuangrui Ding and Songyang Zhang and Haodong Duan and Wenwei Zhang and Hang Yan and Xinyue Zhang and Wei Li and Jingwen Li and Kai Chen and Conghui He and Xingcheng Zhang and Yu Qiao and Dahua Lin and Jiaqi Wang},
      journal={arXiv preprint arXiv:2309.15112},
      year={2023}
}
```

<br>

## è®¸å¯è¯ & è”ç³»æˆ‘ä»¬

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œä¹Ÿå¯ç”³è¯·å…è´¹çš„å•†ä¸šä½¿ç”¨æˆæƒï¼ˆ[ç”³è¯·è¡¨](https://wj.qq.com/s2/12725412/f7c1/)ï¼‰ã€‚å…¶ä»–é—®é¢˜ä¸åˆä½œè¯·è”ç³» <internlm@pjlab.org.cn>ã€‚
